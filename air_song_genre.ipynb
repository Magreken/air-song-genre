{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545f880a",
   "metadata": {},
   "source": [
    "First, we manage all imports for the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a87ebc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: Cython==0.29.32 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (0.29.32)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: pyfume in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: pandas in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: fst-pso in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: simpful in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Requirement already satisfied: requests in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: click in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: torch in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (22.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\danie\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install install -Uq bertopic\n",
    "!pip install torch\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85666458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import ast\n",
    "from torch import nn\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from torch.utils.data import random_split, Subset, DataLoader\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import bertopic\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7c527",
   "metadata": {},
   "source": [
    "Next we process the zipped data to csv. It is stored in .zip, because it is smaller on git and then locally it is extracted to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2edba73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"data/dataset.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")\n",
    "\n",
    "for file_name in os.listdir(\"data/dataset\"):\n",
    "    source = \"data/dataset/\" + file_name\n",
    "    destination = \"data/\" + file_name\n",
    "    if os.path.isfile(source): shutil.move(source, destination)\n",
    "\n",
    "os.rmdir(\"data/dataset\")\n",
    "del source, destination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3a4fc",
   "metadata": {},
   "source": [
    "Preprocessing of the datasets, the goal is to receive a table with the following columns: Name, Lyrics, Genre<br>\n",
    "df Dataset - Name, Lyrics, Genre<br>\n",
    "df3 Dataset - 10000 entrys of random selectet lyrics to train doc2vec\n",
    "df4 Dataset - 28k~ entrys of songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01b36467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished combining..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('data/Spotify-2000.csv')\n",
    "df = df[['Title', 'Top Genre']] #take only the name and genre\n",
    "df2 = pd.read_csv('data/spotify_millsongdata.csv')\n",
    "df4 = pd.read_csv('data/tcc_ceds_music.csv')\n",
    "df4 = df4[['track_name','genre','lyrics']]\n",
    "\n",
    "df['lyrics'] = '' # add column lyrics\n",
    "#Now combine those two documents by the title\n",
    "found = 0\n",
    "\n",
    "for x, title in enumerate(df['Title']):\n",
    "    #print(title)\n",
    "    for y, title2 in enumerate(df2['song']):\n",
    "        if title2.lower() == title.lower():\n",
    "            df['lyrics'].iloc[x] = df2['text'].iloc[y]\n",
    "\n",
    "print(\"finished combining..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e1a368",
   "metadata": {},
   "source": [
    "Collect 40000 random entrys of lyrics from the millsongdata Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40cf936b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "df3 = df2['text'].sample(n=40000)\n",
    "#print(df3)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a98621f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Title          Top Genre  \\\n",
      "0               Sunrise    adult standards   \n",
      "1           Black Night         album rock   \n",
      "3         The Pretender  alternative metal   \n",
      "6     She Will Be Loved                pop   \n",
      "8        Mr. Brightside        modern rock   \n",
      "...                 ...                ...   \n",
      "1988         Summertime    adult standards   \n",
      "1989   Heartbreak Hotel    adult standards   \n",
      "1990          Hound Dog    adult standards   \n",
      "1991    Johnny B. Goode         blues rock   \n",
      "1993     Blueberry Hill    adult standards   \n",
      "\n",
      "                                                 lyrics  \n",
      "0     You take away the breath I was keeping for sun...  \n",
      "1     Black night is not right,  \\r\\nI don't feel so...  \n",
      "3     I'm going to rent myself a house  \\r\\nIn the s...  \n",
      "6     Beauty queen of only eighteen she  \\r\\nHad som...  \n",
      "8     Coming out of my cage  \\r\\nAnd I've been doing...  \n",
      "...                                                 ...  \n",
      "1988  Summertime....and the livin' is easy  \\r\\nFish...  \n",
      "1989  Now, since my baby left me  \\r\\nI've found a n...  \n",
      "1990  You ain't nothing but a hound dog, crying all ...  \n",
      "1991  Way down in Louisiana close to New Orleans  \\r...  \n",
      "1993  I found my thrill on Blueberry Hill  \\r\\nOn Bl...  \n",
      "\n",
      "[895 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#remove songs which were not in both datsets\n",
    "songs_to_remove = []\n",
    "for x, lyrics in enumerate(df['lyrics']):\n",
    "    if lyrics == '':\n",
    "        songs_to_remove.append(x)\n",
    "df.drop(songs_to_remove, axis = 0, inplace = True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e76e7",
   "metadata": {},
   "source": [
    "Further preprocessing of the lyrics itself. We remove the stopwords and punctuations with regex and stopwords from ntlk form df and df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db225637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Title          Top Genre  \\\n",
      "0               Sunrise    adult standards   \n",
      "1           Black Night         album rock   \n",
      "3         The Pretender  alternative metal   \n",
      "6     She Will Be Loved                pop   \n",
      "8        Mr. Brightside        modern rock   \n",
      "...                 ...                ...   \n",
      "1988         Summertime    adult standards   \n",
      "1989   Heartbreak Hotel    adult standards   \n",
      "1990          Hound Dog    adult standards   \n",
      "1991    Johnny B. Goode         blues rock   \n",
      "1993     Blueberry Hill    adult standards   \n",
      "\n",
      "                                                 lyrics  \n",
      "0     take away breath keep sunris appear morn look ...  \n",
      "1     black night right dont feel bright dont care s...  \n",
      "3     im go rent hous shade freeway gonna pack lunch...  \n",
      "6     beauti queen eighteen troubl alway help alway ...  \n",
      "8     come cage ive fine gotta gotta want start kiss...  \n",
      "...                                                 ...  \n",
      "1988  summertimeand livin easi fish jumpinand cotton...  \n",
      "1989  sinc babi left ive found new place dwell end l...  \n",
      "1990  aint noth hound dog cri time aint noth hound d...  \n",
      "1991  way louisiana close new orlean way back wood a...  \n",
      "1993  found thrill blueberri hill blueberri hill fou...  \n",
      "\n",
      "[895 rows x 3 columns]\n",
      "finished preprocessing of df\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "sw = stopwords.words('english')\n",
    "punc_regex = r'[^\\s\\w]' #searches for everything thats not a word or space\n",
    "stopword_regex = r'\\b{0}\\b'\n",
    "space_regex = r'\\s\\s+'\n",
    "newl_regex = r'\\n|\\r'\n",
    "\n",
    "#print(df3.iloc[1])\n",
    "\n",
    "#preprocessing of df\n",
    "for x, lyrics in enumerate(df['lyrics']):\n",
    "    txt = lyrics\n",
    "    txt = re.sub(punc_regex, '', txt) #remove punuctuations\n",
    "    for sword in sw:\n",
    "        txt = re.sub(stopword_regex.format(sword), '', txt, flags=re.IGNORECASE) #remove every stopword\n",
    "    txt = re.sub(newl_regex, '', txt)\n",
    "    stemmed_txt = ''\n",
    "    for word in txt.split():\n",
    "        stemmed_txt += str(stemmer.stem(word.lower())) + \" \"\n",
    "    \n",
    "    df['lyrics'].iloc[x] = stemmed_txt\n",
    "\n",
    "print(df)\n",
    "print('finished preprocessing of df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b5a71f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11389    dont want compani dont need take care let long...\n",
      "48483    morn night stay sight didnt recogn id becom al...\n",
      "19057    cat tabl wait dinner til children came shoo aw...\n",
      "20004    time lotsa money id take round world n dime ev...\n",
      "50840    look hang word everi littl word look stare win...\n",
      "                               ...                        \n",
      "2830     there rain window im think tear pillow come jo...\n",
      "37346    cut away somebodi cut away desper heart cut aw...\n",
      "38732    sometim rememb reason let go insid see face re...\n",
      "53923    dont know old found armor belli sixteenth cent...\n",
      "48720    pogu lorelei told tale love glori old sad song...\n",
      "Name: text, Length: 10000, dtype: object\n",
      "finished preprocessing of df3\n"
     ]
    }
   ],
   "source": [
    "#preprocessing of df3 -> dataset for doc2vec training\n",
    "\n",
    "preprocess = False\n",
    "\n",
    "if(preprocess):\n",
    "    for x, lyrics in enumerate(df3):\n",
    "        txt = lyrics\n",
    "        txt = re.sub(punc_regex, '', txt) #remove punuctuations\n",
    "        for sword in sw:\n",
    "            txt = re.sub(stopword_regex.format(sword), '', txt, flags=re.IGNORECASE) #remove every stopword\n",
    "        txt = re.sub(newl_regex, '', txt)\n",
    "        stemmed_txt = ''\n",
    "        for word in txt.split():\n",
    "            stemmed_txt += str(stemmer.stem(word.lower())) + \" \"\n",
    "\n",
    "        df3.iloc[x] = stemmed_txt\n",
    "    os.remove('data/doc2vec_training_data.csv')\n",
    "    df3.to_csv('data/doc2vec_training_data.csv', index=False)\n",
    "else:\n",
    "    df3 = pd.read_csv('data/doc2vec_training_data.csv')\n",
    "\n",
    "print(df3)\n",
    "print('finished preprocessing of df3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "135a0530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          track_name    genre  \\\n",
      "0               mohabbat bhi jhoothi      pop   \n",
      "1                          i believe      pop   \n",
      "2                                cry      pop   \n",
      "3                           patricia      pop   \n",
      "4                 apopse eida oneiro      pop   \n",
      "...                              ...      ...   \n",
      "28367                10 million ways  hip hop   \n",
      "28368  ante up (robbin hoodz theory)  hip hop   \n",
      "28369                  whutcha want?  hip hop   \n",
      "28370                         switch  hip hop   \n",
      "28371                         r.i.p.  hip hop   \n",
      "\n",
      "                                                  lyrics  \n",
      "0      hold time feel break feel untru convinc speak ...  \n",
      "1      believ drop rain fall grow believ darkest nigh...  \n",
      "2      sweetheart send letter goodby secret feel bett...  \n",
      "3      kiss lip want stroll charm mambo chacha mering...  \n",
      "4      till darl till matter know till dream live apa...  \n",
      "...                                                  ...  \n",
      "28367  caus fuck leav scar tick tock clock come knock...  \n",
      "28368  mink thing chain ring braclet yap fame come fo...  \n",
      "28369  get ban get ban stick crack relax plan attack ...  \n",
      "28370  check check yeah yeah hear thing call switch g...  \n",
      "28371  remix killer aliv remix thriller trap bitch sp...  \n",
      "\n",
      "[28372 rows x 3 columns]\n",
      "finished preprocessing of df4\n"
     ]
    }
   ],
   "source": [
    "#preprocessing of df4\n",
    "for x, lyrics in enumerate(df4['lyrics']):\n",
    "    txt = lyrics\n",
    "    txt = re.sub(punc_regex, '', txt) #remove punuctuations\n",
    "    for sword in sw:\n",
    "        txt = re.sub(stopword_regex.format(sword), '', txt, flags=re.IGNORECASE) #remove every stopword\n",
    "    \n",
    "    txt = re.sub(newl_regex, '', txt)\n",
    "    stemmed_txt = ''\n",
    "    for word in txt.split():\n",
    "        stemmed_txt += str(stemmer.stem(word.lower())) + \" \"\n",
    "    df4['lyrics'].iloc[x] = stemmed_txt\n",
    "\n",
    "print(df4)\n",
    "print('finished preprocessing of df4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43056ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "#display(df)\n",
    "\n",
    "topic_model = bertopic.BERTopic(language='english', embedding_model=sentence_model,verbose=True)\n",
    "\n",
    "topics, probs = topic_model.fit_transform(df[\"lyrics\"])\n",
    "\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad17c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_model.get_topic_info())\n",
    "\n",
    "print(topic_model.get_topics()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded6eae",
   "metadata": {},
   "source": [
    "Tokenize the lyrics and create tagged Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39f23826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14988\n"
     ]
    }
   ],
   "source": [
    "tagged_data = []\n",
    "nltk.download('punkt')\n",
    "\n",
    "for i,d in enumerate(df3):\n",
    "    tokenized_words = nltk.tokenize.word_tokenize(d)\n",
    "    tagged_data.append(TaggedDocument(words=tokenized_words, tags=str(i)))\n",
    "\n",
    "print(len(tagged_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982a54b",
   "metadata": {},
   "source": [
    "Setting up the Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52e8276d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec model finished training\n"
     ]
    }
   ],
   "source": [
    "doc2vec_model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=1, epochs=30)\n",
    "doc2vec_model.build_vocab(tagged_data)\n",
    "doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n",
    "print(\"Doc2Vec model finished training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96c0677",
   "metadata": {},
   "source": [
    "Building the DataLoader for the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b04cd0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop 7042\n",
      "country 5445\n",
      "blues 4604\n",
      "jazz 3845\n",
      "reggae 2498\n",
      "rock 4034\n",
      "2498\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_name</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love hot nights</td>\n",
       "      <td>rock</td>\n",
       "      <td>night tshirt feel right stay later outsid stay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pressure</td>\n",
       "      <td>reggae</td>\n",
       "      <td>special dedic peopl live poverti direct peopl ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inject the venom</td>\n",
       "      <td>rock</td>\n",
       "      <td>want plead need tell truth tell lie cross hear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hanging fire</td>\n",
       "      <td>reggae</td>\n",
       "      <td>yeah come life come life come life babi come t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lavish</td>\n",
       "      <td>jazz</td>\n",
       "      <td>drive insan complain lose word needl day samei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14983</th>\n",
       "      <td>what is this thing called love?</td>\n",
       "      <td>blues</td>\n",
       "      <td>thing call funni thing call solv mysteri fool ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14984</th>\n",
       "      <td>nineteen somethin'</td>\n",
       "      <td>country</td>\n",
       "      <td>bathroom floor night dread shame disgrac prett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14985</th>\n",
       "      <td>my gift to you</td>\n",
       "      <td>rock</td>\n",
       "      <td>precious long hide shadow break soul want tell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14986</th>\n",
       "      <td>children of tomorrow</td>\n",
       "      <td>reggae</td>\n",
       "      <td>bargain think children tomorrow tell search st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14987</th>\n",
       "      <td>burning of the midnight lamp</td>\n",
       "      <td>blues</td>\n",
       "      <td>morn dead leav lead moon loneli felt today lit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14988 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            track_name    genre  \\\n",
       "0                    i love hot nights     rock   \n",
       "1                             pressure   reggae   \n",
       "2                     inject the venom     rock   \n",
       "3                         hanging fire   reggae   \n",
       "4                               lavish     jazz   \n",
       "...                                ...      ...   \n",
       "14983  what is this thing called love?    blues   \n",
       "14984               nineteen somethin'  country   \n",
       "14985                   my gift to you     rock   \n",
       "14986             children of tomorrow   reggae   \n",
       "14987     burning of the midnight lamp    blues   \n",
       "\n",
       "                                                  lyrics  \n",
       "0      night tshirt feel right stay later outsid stay...  \n",
       "1      special dedic peopl live poverti direct peopl ...  \n",
       "2      want plead need tell truth tell lie cross hear...  \n",
       "3      yeah come life come life come life babi come t...  \n",
       "4      drive insan complain lose word needl day samei...  \n",
       "...                                                  ...  \n",
       "14983  thing call funni thing call solv mysteri fool ...  \n",
       "14984  bathroom floor night dread shame disgrac prett...  \n",
       "14985  precious long hide shadow break soul want tell...  \n",
       "14986  bargain think children tomorrow tell search st...  \n",
       "14987  morn dead leav lead moon loneli felt today lit...  \n",
       "\n",
       "[14988 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df4 = df4[df4.genre != \"hip hop\"]\n",
    "\n",
    "pre_norm_genre_dict = {}\n",
    "for i in df4['genre']:\n",
    "    if i in pre_norm_genre_dict:\n",
    "        pre_norm_genre_dict[i] += 1\n",
    "    else:\n",
    "        pre_norm_genre_dict[i] = 1\n",
    "\n",
    "for i in pre_norm_genre_dict:\n",
    "    print(i, pre_norm_genre_dict[i])\n",
    "\n",
    "min_value = min(pre_norm_genre_dict.values())\n",
    "print(min_value)\n",
    "\n",
    "new_df = df4.groupby('genre').sample(min_value)\n",
    "\n",
    "new_df = new_df.sample(frac = 1)\n",
    "new_df.reset_index(inplace = True,drop = True)\n",
    "display(new_df)\n",
    "df4 = new_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5778176",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Token'] + ['Target']\n",
    "df_for_dataloader = pd.DataFrame(columns = columns)\n",
    "df_for_dataloader.set_index(columns)\n",
    "\n",
    "lookup_dict = {}\n",
    "token_list = []\n",
    "target_list = []\n",
    "i = 0\n",
    "for d in df4[\"genre\"]:\n",
    "    if d not in lookup_dict:\n",
    "        lookup_dict[d] = i\n",
    "        i+=1\n",
    "\n",
    "for index, df_row in df4.iterrows():\n",
    "    lyrics_tokenized = nltk.tokenize.word_tokenize(df_row[\"lyrics\"])\n",
    "    token = [doc2vec_model.infer_vector(lyrics_tokenized)]\n",
    "    #print(token)\n",
    "    one_hot_encoded_vector = []\n",
    "    for x in lookup_dict.keys():\n",
    "        if df_row[\"genre\"] == x:\n",
    "            one_hot_encoded_vector.append(1)\n",
    "        else:\n",
    "            one_hot_encoded_vector.append(0)\n",
    "    target = [np.array(one_hot_encoded_vector)] # should be genre\n",
    "    #row = pd.DataFrame([token + target], columns=['Token', 'Target'])\n",
    "    #df_for_dataloader = pd.concat([df_for_dataloader, row])\n",
    "    token_list.append(token)\n",
    "    target_list.append(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8b7b4",
   "metadata": {},
   "source": [
    "Splitting the dataset into train and test (80,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8197733e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11991\n",
      "2997\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = torch.utils.data.random_split(list(zip(token_list, target_list)), [0.8, 0.2])\n",
    "print(len(data_train))\n",
    "print(len(data_test))\n",
    "    \n",
    "dataloader_train = torch.utils.data.DataLoader(data_train, batch_size=32)\n",
    "dataloader_test = torch.utils.data.DataLoader(data_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b0da18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 481\n",
      "3 507\n",
      "0 487\n",
      "5 509\n",
      "1 496\n",
      "4 517\n"
     ]
    }
   ],
   "source": [
    "genre_test_count_dict = {}\n",
    "for i in data_test:\n",
    "  if np.argmax(i[1]) in genre_test_count_dict:\n",
    "    genre_test_count_dict[np.argmax(i[1])] += 1\n",
    "  else:\n",
    "    genre_test_count_dict[np.argmax(i[1])] = 1\n",
    "\n",
    "genre_train_count_dict = {}\n",
    "for i in data_train:\n",
    "  if np.argmax(i[1]) in genre_train_count_dict:\n",
    "    genre_train_count_dict[np.argmax(i[1])] += 1\n",
    "  else:\n",
    "    genre_train_count_dict[np.argmax(i[1])] = 1\n",
    "\n",
    "for i in genre_test_count_dict:\n",
    "  print(i, genre_test_count_dict[i])\n",
    "\n",
    "lookup_dict_reverse = {y: x for x, y in lookup_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde96070",
   "metadata": {},
   "source": [
    "We define out neurol network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b771ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu') #torch.device('cuda' if torch.cuda.is_available() else 'cpu') For faster training in the end\n",
    "\n",
    "class SongModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SongModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layer_1 =   nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.layer_2 =   nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.layer_out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax =   nn.Softmax()\n",
    "        self.sigmoid =   nn.Sigmoid()\n",
    "        self.tanh =      nn.Tanh()\n",
    "\n",
    "        nn.init.xavier_uniform_(self.layer_1.weight)\n",
    "        nn.init.zeros_(self.layer_1.bias)\n",
    "        nn.init.xavier_uniform_(self.layer_out.weight)\n",
    "        nn.init.zeros_(self.layer_out.bias)\n",
    "\n",
    "\n",
    "    def forward(self, d):\n",
    "        x = self.sigmoid(self.layer_1(d))\n",
    "        x = self.softmax(self.layer_out(x))\n",
    "        return x\n",
    "\n",
    "model = SongModel(dataloader_train.dataset[0][0][0].size, dataloader_train.dataset[0][1][0].size).to(device)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276ef344",
   "metadata": {},
   "source": [
    "Define lists to save the accuracy and loss in each iteration to plot them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4b4caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_normal_model = []\n",
    "acc_test_normal_model = []\n",
    "loss_train_normal_model = []\n",
    "loss_test_normal_model = []\n",
    "genre_acc_train_normal_model = []\n",
    "genre_acc_test_normal_model = []\n",
    "for i in range(len(lookup_dict)):\n",
    "    genre_acc_train_normal_model.append([])\n",
    "    genre_acc_test_normal_model.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40162d72",
   "metadata": {},
   "source": [
    "Here is the training and test function for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10d742ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer,loss_fn):\n",
    "    total_loss = 0\n",
    "    acc = 0\n",
    "    genre_accuracy = {}\n",
    "    for x, key in enumerate(lookup_dict):\n",
    "        genre_accuracy[x] = 0\n",
    "\n",
    "    for sample in dataloader:\n",
    "        model_input = sample[0][0]\n",
    "        should = sample[1][0]\n",
    "        predict = model(model_input)\n",
    "        for sample_index in range(len(should)):\n",
    "            predict_e = predict[sample_index].data.numpy()\n",
    "            should_e = should[sample_index].numpy()\n",
    "            if np.where(predict_e == np.amax(predict_e))[0][0] == np.where(should_e == np.amax(should_e))[0][0]:\n",
    "                acc += 1\n",
    "                genre_accuracy[np.where(should_e == np.amax(should_e))[0][0]] += 1\n",
    "        loss = loss_fn(predict, should.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "    loss_train_normal_model.append(total_loss.detach().numpy() / len(dataloader.dataset))\n",
    "    acc_train_normal_model.append(acc * 100 / len(dataloader.dataset))\n",
    "    print(f\"\\tTrain - \\tLoss: {loss_train_normal_model[-1]:3.10}, Acc: {acc_train_normal_model[-1]:3.5}%\", end=\"\\n\")\n",
    "    for (genre, id_genre) in lookup_dict.items():\n",
    "        genre_acc_train_normal_model[id_genre].append(round(genre_accuracy[lookup_dict[genre]] * 100 / genre_train_count_dict[id_genre],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d220ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, loss_fn):\n",
    "    total_loss = 0\n",
    "    acc = 0\n",
    "    genre_accuracy = {}\n",
    "    for x, key in enumerate(lookup_dict):\n",
    "        genre_accuracy[x] = 0\n",
    "\n",
    "    for sample in dataloader:\n",
    "        model_input = sample[0][0]\n",
    "        should = sample[1][0]\n",
    "        predict = model(model_input)\n",
    "        for sample_index in range(len(should)):\n",
    "            predict_e = predict[sample_index].data.numpy()\n",
    "            should_e = should[sample_index].numpy()\n",
    "            if np.where(predict_e == np.amax(predict_e))[0][0] == np.where(should_e == np.amax(should_e))[0][0]:\n",
    "                acc += 1\n",
    "                genre_accuracy[np.where(should_e == np.amax(should_e))[0][0]] += 1\n",
    "\n",
    "        loss = loss_fn(predict, should.float())\n",
    "        total_loss += loss\n",
    "\n",
    "    loss_test_normal_model.append(total_loss.detach().numpy() / len(dataloader.dataset))\n",
    "    acc_test_normal_model.append(acc * 100 / len(dataloader.dataset))\n",
    "    print(f\"\\tEvaluation - \\tLoss: {loss_test_normal_model[-1]:3.10}, Acc: {acc_test_normal_model[-1]:3.5}%\", end=\"\\n\")\n",
    "    for (genre, id_genre) in lookup_dict.items():\n",
    "        genre_acc_test_normal_model[id_genre].append(round(genre_accuracy[lookup_dict[genre]] * 100 / genre_test_count_dict[id_genre],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1ebde06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "\tTrain - \tLoss: 0.05555806157, Acc: 22.425%\n",
      "\tEvaluation - \tLoss: 0.05557510636, Acc: 23.991%\n",
      "Epoch 1:\n",
      "\tTrain - \tLoss: 0.05520167958, Acc: 23.926%\n",
      "\tEvaluation - \tLoss: 0.05541953493, Acc: 25.058%\n",
      "Epoch 2:\n",
      "\tTrain - \tLoss: 0.05504256875, Acc: 25.06%\n",
      "\tEvaluation - \tLoss: 0.05532046227, Acc: 25.592%\n",
      "Epoch 3:\n",
      "\tTrain - \tLoss: 0.05492659127, Acc: 25.719%\n",
      "\tEvaluation - \tLoss: 0.05524668345, Acc: 25.626%\n",
      "Epoch 4:\n",
      "\tTrain - \tLoss: 0.0548339518, Acc: 26.22%\n",
      "\tEvaluation - \tLoss: 0.05518813797, Acc: 26.226%\n",
      "Epoch 5:\n",
      "\tTrain - \tLoss: 0.05475710176, Acc: 26.495%\n",
      "\tEvaluation - \tLoss: 0.05514000939, Acc: 26.426%\n",
      "Epoch 6:\n",
      "\tTrain - \tLoss: 0.05469181639, Acc: 26.728%\n",
      "\tEvaluation - \tLoss: 0.05509950767, Acc: 26.894%\n",
      "Epoch 7:\n",
      "\tTrain - \tLoss: 0.0546353114, Acc: 26.979%\n",
      "\tEvaluation - \tLoss: 0.05506484573, Acc: 27.06%\n",
      "Epoch 8:\n",
      "\tTrain - \tLoss: 0.05458580019, Acc: 27.22%\n",
      "\tEvaluation - \tLoss: 0.05503478637, Acc: 27.227%\n",
      "Epoch 9:\n",
      "\tTrain - \tLoss: 0.05454178626, Acc: 27.404%\n",
      "\tEvaluation - \tLoss: 0.05500843352, Acc: 27.227%\n",
      "Epoch 10:\n",
      "\tTrain - \tLoss: 0.05450222616, Acc: 27.729%\n",
      "\tEvaluation - \tLoss: 0.05498511512, Acc: 26.927%\n",
      "Epoch 11:\n",
      "\tTrain - \tLoss: 0.05446645307, Acc: 27.913%\n",
      "\tEvaluation - \tLoss: 0.05496430166, Acc: 26.793%\n",
      "Epoch 12:\n",
      "\tTrain - \tLoss: 0.05443372894, Acc: 27.946%\n",
      "\tEvaluation - \tLoss: 0.05494561639, Acc: 26.76%\n",
      "Epoch 13:\n",
      "\tTrain - \tLoss: 0.05440361603, Acc: 28.104%\n",
      "\tEvaluation - \tLoss: 0.05492872328, Acc: 26.426%\n",
      "Epoch 14:\n",
      "\tTrain - \tLoss: 0.0543757631, Acc: 28.238%\n",
      "\tEvaluation - \tLoss: 0.05491338812, Acc: 26.393%\n",
      "Epoch 15:\n",
      "\tTrain - \tLoss: 0.05434990041, Acc: 28.38%\n",
      "\tEvaluation - \tLoss: 0.05489940726, Acc: 26.493%\n",
      "Epoch 16:\n",
      "\tTrain - \tLoss: 0.05432570725, Acc: 28.396%\n",
      "\tEvaluation - \tLoss: 0.0548866076, Acc: 26.426%\n",
      "Epoch 17:\n",
      "\tTrain - \tLoss: 0.05430299022, Acc: 28.58%\n",
      "\tEvaluation - \tLoss: 0.0548748262, Acc: 26.493%\n",
      "Epoch 18:\n",
      "\tTrain - \tLoss: 0.05428161188, Acc: 28.68%\n",
      "\tEvaluation - \tLoss: 0.05486398162, Acc: 26.393%\n",
      "Epoch 19:\n",
      "\tTrain - \tLoss: 0.0542614399, Acc: 28.638%\n",
      "\tEvaluation - \tLoss: 0.05485396183, Acc: 26.36%\n",
      "Epoch 20:\n",
      "\tTrain - \tLoss: 0.05424233173, Acc: 28.722%\n",
      "\tEvaluation - \tLoss: 0.05484464974, Acc: 26.46%\n",
      "Epoch 21:\n",
      "\tTrain - \tLoss: 0.05422415505, Acc: 28.763%\n",
      "\tEvaluation - \tLoss: 0.05483604536, Acc: 26.527%\n",
      "Epoch 22:\n",
      "\tTrain - \tLoss: 0.05420684878, Acc: 28.847%\n",
      "\tEvaluation - \tLoss: 0.05482800102, Acc: 26.56%\n",
      "Epoch 23:\n",
      "\tTrain - \tLoss: 0.05419035692, Acc: 28.872%\n",
      "\tEvaluation - \tLoss: 0.05482057273, Acc: 26.56%\n",
      "Epoch 24:\n",
      "\tTrain - \tLoss: 0.0541746133, Acc: 28.922%\n",
      "\tEvaluation - \tLoss: 0.05481357721, Acc: 26.66%\n",
      "Epoch 25:\n",
      "\tTrain - \tLoss: 0.05415950594, Acc: 29.005%\n",
      "\tEvaluation - \tLoss: 0.05480709082, Acc: 26.693%\n",
      "Epoch 26:\n",
      "\tTrain - \tLoss: 0.05414507556, Acc: 29.122%\n",
      "\tEvaluation - \tLoss: 0.05480101175, Acc: 26.86%\n",
      "Epoch 27:\n",
      "\tTrain - \tLoss: 0.05413114401, Acc: 29.114%\n",
      "\tEvaluation - \tLoss: 0.0547953247, Acc: 26.76%\n",
      "Epoch 28:\n",
      "\tTrain - \tLoss: 0.05411783345, Acc: 29.272%\n",
      "\tEvaluation - \tLoss: 0.05478999915, Acc: 26.827%\n",
      "Epoch 29:\n",
      "\tTrain - \tLoss: 0.05410498609, Acc: 29.28%\n",
      "\tEvaluation - \tLoss: 0.05478502998, Acc: 26.827%\n",
      "Epoch 30:\n",
      "\tTrain - \tLoss: 0.05409259174, Acc: 29.347%\n",
      "\tEvaluation - \tLoss: 0.05478033066, Acc: 26.727%\n",
      "Epoch 31:\n",
      "\tTrain - \tLoss: 0.05408064023, Acc: 29.289%\n",
      "\tEvaluation - \tLoss: 0.05477596737, Acc: 26.727%\n",
      "Epoch 32:\n",
      "\tTrain - \tLoss: 0.0540691621, Acc: 29.289%\n",
      "\tEvaluation - \tLoss: 0.05477185356, Acc: 26.66%\n",
      "Epoch 33:\n",
      "\tTrain - \tLoss: 0.05405799955, Acc: 29.305%\n",
      "\tEvaluation - \tLoss: 0.05476801468, Acc: 26.827%\n",
      "Epoch 34:\n",
      "\tTrain - \tLoss: 0.05404723403, Acc: 29.322%\n",
      "\tEvaluation - \tLoss: 0.05476438963, Acc: 26.96%\n",
      "Epoch 35:\n",
      "\tTrain - \tLoss: 0.05403684009, Acc: 29.439%\n",
      "\tEvaluation - \tLoss: 0.05476100897, Acc: 26.927%\n",
      "Epoch 36:\n",
      "\tTrain - \tLoss: 0.05402675664, Acc: 29.514%\n",
      "\tEvaluation - \tLoss: 0.0547578116, Acc: 26.894%\n",
      "Epoch 37:\n",
      "\tTrain - \tLoss: 0.05401699387, Acc: 29.53%\n",
      "\tEvaluation - \tLoss: 0.05475483316, Acc: 26.86%\n",
      "Epoch 38:\n",
      "\tTrain - \tLoss: 0.05400748559, Acc: 29.555%\n",
      "\tEvaluation - \tLoss: 0.05475200746, Acc: 26.827%\n",
      "Epoch 39:\n",
      "\tTrain - \tLoss: 0.05399829291, Acc: 29.555%\n",
      "\tEvaluation - \tLoss: 0.05474938032, Acc: 26.76%\n",
      "Epoch 40:\n",
      "\tTrain - \tLoss: 0.05398938527, Acc: 29.631%\n",
      "\tEvaluation - \tLoss: 0.05474692629, Acc: 26.693%\n",
      "Epoch 41:\n",
      "\tTrain - \tLoss: 0.05398070159, Acc: 29.647%\n",
      "\tEvaluation - \tLoss: 0.05474459445, Acc: 26.593%\n",
      "Epoch 42:\n",
      "\tTrain - \tLoss: 0.05397224697, Acc: 29.722%\n",
      "\tEvaluation - \tLoss: 0.05474242553, Acc: 26.56%\n",
      "Epoch 43:\n",
      "\tTrain - \tLoss: 0.05396406212, Acc: 29.714%\n",
      "\tEvaluation - \tLoss: 0.05474039917, Acc: 26.493%\n",
      "Epoch 44:\n",
      "\tTrain - \tLoss: 0.05395609614, Acc: 29.739%\n",
      "\tEvaluation - \tLoss: 0.05473848482, Acc: 26.426%\n",
      "Epoch 45:\n",
      "\tTrain - \tLoss: 0.05394827778, Acc: 29.806%\n",
      "\tEvaluation - \tLoss: 0.05473669267, Acc: 26.36%\n",
      "Epoch 46:\n",
      "\tTrain - \tLoss: 0.05394071392, Acc: 29.856%\n",
      "\tEvaluation - \tLoss: 0.05473501761, Acc: 26.393%\n",
      "Epoch 47:\n",
      "\tTrain - \tLoss: 0.05393330785, Acc: 29.856%\n",
      "\tEvaluation - \tLoss: 0.05473346984, Acc: 26.46%\n",
      "Epoch 48:\n",
      "\tTrain - \tLoss: 0.05392613084, Acc: 29.947%\n",
      "\tEvaluation - \tLoss: 0.05473201371, Acc: 26.527%\n",
      "Epoch 49:\n",
      "\tTrain - \tLoss: 0.05391910144, Acc: 29.922%\n",
      "\tEvaluation - \tLoss: 0.05473066959, Acc: 26.527%\n",
      "Epoch 50:\n",
      "\tTrain - \tLoss: 0.05391225019, Acc: 29.964%\n",
      "\tEvaluation - \tLoss: 0.05472938657, Acc: 26.393%\n",
      "Epoch 51:\n",
      "\tTrain - \tLoss: 0.05390553128, Acc: 29.997%\n",
      "\tEvaluation - \tLoss: 0.05472821047, Acc: 26.36%\n",
      "Epoch 52:\n",
      "\tTrain - \tLoss: 0.05389901598, Acc: 30.006%\n",
      "\tEvaluation - \tLoss: 0.05472712092, Acc: 26.326%\n",
      "Epoch 53:\n",
      "\tTrain - \tLoss: 0.05389262284, Acc: 30.039%\n",
      "\tEvaluation - \tLoss: 0.0547260721, Acc: 26.326%\n",
      "Epoch 54:\n",
      "\tTrain - \tLoss: 0.05388634677, Acc: 30.064%\n",
      "\tEvaluation - \tLoss: 0.05472513529, Acc: 26.293%\n",
      "Epoch 55:\n",
      "\tTrain - \tLoss: 0.05388022849, Acc: 30.048%\n",
      "\tEvaluation - \tLoss: 0.05472425449, Acc: 26.293%\n",
      "Epoch 56:\n",
      "\tTrain - \tLoss: 0.0538742731, Acc: 30.089%\n",
      "\tEvaluation - \tLoss: 0.05472343478, Acc: 26.326%\n",
      "Epoch 57:\n",
      "\tTrain - \tLoss: 0.05386844495, Acc: 30.106%\n",
      "\tEvaluation - \tLoss: 0.05472268126, Acc: 26.36%\n",
      "Epoch 58:\n",
      "\tTrain - \tLoss: 0.05386268807, Acc: 30.081%\n",
      "\tEvaluation - \tLoss: 0.05472199393, Acc: 26.36%\n",
      "Epoch 59:\n",
      "\tTrain - \tLoss: 0.05385705844, Acc: 30.106%\n",
      "\tEvaluation - \tLoss: 0.05472136769, Acc: 26.36%\n",
      "Epoch 60:\n",
      "\tTrain - \tLoss: 0.05385156625, Acc: 30.081%\n",
      "\tEvaluation - \tLoss: 0.05472079746, Acc: 26.393%\n",
      "Epoch 61:\n",
      "\tTrain - \tLoss: 0.05384617076, Acc: 30.089%\n",
      "\tEvaluation - \tLoss: 0.05472027814, Acc: 26.293%\n",
      "Epoch 62:\n",
      "\tTrain - \tLoss: 0.05384086181, Acc: 30.106%\n",
      "\tEvaluation - \tLoss: 0.05471979955, Acc: 26.226%\n",
      "Epoch 63:\n",
      "\tTrain - \tLoss: 0.05383569028, Acc: 30.123%\n",
      "\tEvaluation - \tLoss: 0.05471937188, Acc: 26.226%\n",
      "Epoch 64:\n",
      "\tTrain - \tLoss: 0.05383061038, Acc: 30.181%\n",
      "\tEvaluation - \tLoss: 0.05471899003, Acc: 26.226%\n",
      "Epoch 65:\n",
      "\tTrain - \tLoss: 0.05382558138, Acc: 30.223%\n",
      "\tEvaluation - \tLoss: 0.05471864891, Acc: 26.193%\n",
      "Epoch 66:\n",
      "\tTrain - \tLoss: 0.05382070509, Acc: 30.239%\n",
      "\tEvaluation - \tLoss: 0.05471834852, Acc: 26.193%\n",
      "Epoch 67:\n",
      "\tTrain - \tLoss: 0.05381588987, Acc: 30.214%\n",
      "\tEvaluation - \tLoss: 0.05471807867, Acc: 26.159%\n",
      "Epoch 68:\n",
      "\tTrain - \tLoss: 0.05381116119, Acc: 30.198%\n",
      "\tEvaluation - \tLoss: 0.05471785975, Acc: 26.193%\n",
      "Epoch 69:\n",
      "\tTrain - \tLoss: 0.05380651903, Acc: 30.214%\n",
      "\tEvaluation - \tLoss: 0.05471767137, Acc: 26.159%\n",
      "Epoch 70:\n",
      "\tTrain - \tLoss: 0.05380194305, Acc: 30.206%\n",
      "\tEvaluation - \tLoss: 0.05471751353, Acc: 26.26%\n",
      "Epoch 71:\n",
      "\tTrain - \tLoss: 0.05379744851, Acc: 30.181%\n",
      "\tEvaluation - \tLoss: 0.05471739134, Acc: 26.26%\n",
      "Epoch 72:\n",
      "\tTrain - \tLoss: 0.05379301505, Acc: 30.189%\n",
      "\tEvaluation - \tLoss: 0.05471727424, Acc: 26.326%\n",
      "Epoch 73:\n",
      "\tTrain - \tLoss: 0.05378870375, Acc: 30.198%\n",
      "\tEvaluation - \tLoss: 0.05471721314, Acc: 26.293%\n",
      "Epoch 74:\n",
      "\tTrain - \tLoss: 0.05378443826, Acc: 30.214%\n",
      "\tEvaluation - \tLoss: 0.05471719278, Acc: 26.293%\n",
      "Epoch 75:\n",
      "\tTrain - \tLoss: 0.05378022368, Acc: 30.239%\n",
      "\tEvaluation - \tLoss: 0.05471718769, Acc: 26.26%\n",
      "Epoch 76:\n",
      "\tTrain - \tLoss: 0.05377608544, Acc: 30.223%\n",
      "\tEvaluation - \tLoss: 0.0547171775, Acc: 26.193%\n",
      "Epoch 77:\n",
      "\tTrain - \tLoss: 0.05377199302, Acc: 30.264%\n",
      "\tEvaluation - \tLoss: 0.0547172386, Acc: 26.193%\n",
      "Epoch 78:\n",
      "\tTrain - \tLoss: 0.05376801257, Acc: 30.281%\n",
      "\tEvaluation - \tLoss: 0.05471728951, Acc: 26.126%\n",
      "Epoch 79:\n",
      "\tTrain - \tLoss: 0.0537640474, Acc: 30.298%\n",
      "\tEvaluation - \tLoss: 0.0547173557, Acc: 26.126%\n",
      "Epoch 80:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain - \tLoss: 0.05376015349, Acc: 30.348%\n",
      "\tEvaluation - \tLoss: 0.0547174728, Acc: 26.126%\n",
      "Epoch 81:\n",
      "\tTrain - \tLoss: 0.05375633593, Acc: 30.323%\n",
      "\tEvaluation - \tLoss: 0.05471760518, Acc: 26.159%\n",
      "Epoch 82:\n",
      "\tTrain - \tLoss: 0.05375252346, Acc: 30.348%\n",
      "\tEvaluation - \tLoss: 0.05471773755, Acc: 26.159%\n",
      "Epoch 83:\n",
      "\tTrain - \tLoss: 0.05374881788, Acc: 30.389%\n",
      "\tEvaluation - \tLoss: 0.05471790048, Acc: 26.159%\n",
      "Epoch 84:\n",
      "\tTrain - \tLoss: 0.0537451632, Acc: 30.414%\n",
      "\tEvaluation - \tLoss: 0.05471805831, Acc: 26.193%\n",
      "Epoch 85:\n",
      "\tTrain - \tLoss: 0.05374154924, Acc: 30.423%\n",
      "\tEvaluation - \tLoss: 0.05471826705, Acc: 26.159%\n",
      "Epoch 86:\n",
      "\tTrain - \tLoss: 0.05373799128, Acc: 30.423%\n",
      "\tEvaluation - \tLoss: 0.05471847071, Acc: 26.159%\n",
      "Epoch 87:\n",
      "\tTrain - \tLoss: 0.05373444858, Acc: 30.431%\n",
      "\tEvaluation - \tLoss: 0.05471868964, Acc: 26.159%\n",
      "Epoch 88:\n",
      "\tTrain - \tLoss: 0.0537310026, Acc: 30.448%\n",
      "\tEvaluation - \tLoss: 0.05471893402, Acc: 26.193%\n",
      "Epoch 89:\n",
      "\tTrain - \tLoss: 0.05372758715, Acc: 30.439%\n",
      "\tEvaluation - \tLoss: 0.05471920386, Acc: 26.26%\n",
      "Epoch 90:\n",
      "\tTrain - \tLoss: 0.05372416662, Acc: 30.456%\n",
      "\tEvaluation - \tLoss: 0.05471944825, Acc: 26.226%\n",
      "Epoch 91:\n",
      "\tTrain - \tLoss: 0.05372085807, Acc: 30.456%\n",
      "\tEvaluation - \tLoss: 0.05471974355, Acc: 26.26%\n",
      "Epoch 92:\n",
      "\tTrain - \tLoss: 0.05371758006, Acc: 30.506%\n",
      "\tEvaluation - \tLoss: 0.05472003375, Acc: 26.193%\n",
      "Epoch 93:\n",
      "\tTrain - \tLoss: 0.05371430713, Acc: 30.523%\n",
      "\tEvaluation - \tLoss: 0.05472033924, Acc: 26.093%\n",
      "Epoch 94:\n",
      "\tTrain - \tLoss: 0.05371110547, Acc: 30.531%\n",
      "\tEvaluation - \tLoss: 0.05472063963, Acc: 26.093%\n",
      "Epoch 95:\n",
      "\tTrain - \tLoss: 0.05370791908, Acc: 30.54%\n",
      "\tEvaluation - \tLoss: 0.05472096547, Acc: 26.059%\n",
      "Epoch 96:\n",
      "\tTrain - \tLoss: 0.05370483958, Acc: 30.523%\n",
      "\tEvaluation - \tLoss: 0.05472129641, Acc: 25.993%\n",
      "Epoch 97:\n",
      "\tTrain - \tLoss: 0.05370170409, Acc: 30.556%\n",
      "\tEvaluation - \tLoss: 0.05472165281, Acc: 25.993%\n",
      "Epoch 98:\n",
      "\tTrain - \tLoss: 0.05369866023, Acc: 30.556%\n",
      "\tEvaluation - \tLoss: 0.0547220092, Acc: 25.993%\n",
      "Epoch 99:\n",
      "\tTrain - \tLoss: 0.05369564181, Acc: 30.615%\n",
      "\tEvaluation - \tLoss: 0.05472237578, Acc: 26.093%\n",
      "Epoch 100:\n",
      "\tTrain - \tLoss: 0.05369266411, Acc: 30.59%\n",
      "\tEvaluation - \tLoss: 0.05472273726, Acc: 26.126%\n",
      "Epoch 101:\n",
      "\tTrain - \tLoss: 0.0536896915, Acc: 30.598%\n",
      "\tEvaluation - \tLoss: 0.05472310893, Acc: 26.026%\n",
      "Epoch 102:\n",
      "\tTrain - \tLoss: 0.05368678507, Acc: 30.598%\n",
      "\tEvaluation - \tLoss: 0.05472350097, Acc: 26.059%\n",
      "Epoch 103:\n",
      "\tTrain - \tLoss: 0.0536838939, Acc: 30.606%\n",
      "\tEvaluation - \tLoss: 0.054723893, Acc: 25.959%\n",
      "Epoch 104:\n",
      "\tTrain - \tLoss: 0.05368105364, Acc: 30.623%\n",
      "\tEvaluation - \tLoss: 0.05472429013, Acc: 25.926%\n",
      "Epoch 105:\n",
      "\tTrain - \tLoss: 0.05367823882, Acc: 30.64%\n",
      "\tEvaluation - \tLoss: 0.05472472289, Acc: 25.859%\n",
      "Epoch 106:\n",
      "\tTrain - \tLoss: 0.05367543419, Acc: 30.648%\n",
      "\tEvaluation - \tLoss: 0.05472512002, Acc: 25.826%\n",
      "Epoch 107:\n",
      "\tTrain - \tLoss: 0.05367271099, Acc: 30.648%\n",
      "\tEvaluation - \tLoss: 0.0547255426, Acc: 25.893%\n",
      "Epoch 108:\n",
      "\tTrain - \tLoss: 0.0536699369, Acc: 30.681%\n",
      "\tEvaluation - \tLoss: 0.05472598046, Acc: 25.826%\n",
      "Epoch 109:\n",
      "\tTrain - \tLoss: 0.05366727479, Acc: 30.715%\n",
      "\tEvaluation - \tLoss: 0.05472641322, Acc: 25.826%\n",
      "Epoch 110:\n",
      "\tTrain - \tLoss: 0.05366461777, Acc: 30.731%\n",
      "\tEvaluation - \tLoss: 0.0547268409, Acc: 25.826%\n",
      "Epoch 111:\n",
      "\tTrain - \tLoss: 0.05366193529, Acc: 30.723%\n",
      "\tEvaluation - \tLoss: 0.0547273093, Acc: 25.826%\n",
      "Epoch 112:\n",
      "\tTrain - \tLoss: 0.05365932917, Acc: 30.74%\n",
      "\tEvaluation - \tLoss: 0.05472774206, Acc: 25.826%\n",
      "Epoch 113:\n",
      "\tTrain - \tLoss: 0.05365675868, Acc: 30.723%\n",
      "\tEvaluation - \tLoss: 0.05472821047, Acc: 25.826%\n",
      "Epoch 114:\n",
      "\tTrain - \tLoss: 0.05365421364, Acc: 30.706%\n",
      "\tEvaluation - \tLoss: 0.05472865851, Acc: 25.826%\n",
      "Epoch 115:\n",
      "\tTrain - \tLoss: 0.0536516686, Acc: 30.706%\n",
      "\tEvaluation - \tLoss: 0.05472911673, Acc: 25.826%\n",
      "Epoch 116:\n",
      "\tTrain - \tLoss: 0.05364912865, Acc: 30.723%\n",
      "\tEvaluation - \tLoss: 0.05472959532, Acc: 25.759%\n",
      "Epoch 117:\n",
      "\tTrain - \tLoss: 0.05364666505, Acc: 30.731%\n",
      "\tEvaluation - \tLoss: 0.05473006372, Acc: 25.726%\n",
      "Epoch 118:\n",
      "\tTrain - \tLoss: 0.05364420145, Acc: 30.731%\n",
      "\tEvaluation - \tLoss: 0.05473054231, Acc: 25.759%\n",
      "Epoch 119:\n",
      "\tTrain - \tLoss: 0.05364175821, Acc: 30.74%\n",
      "\tEvaluation - \tLoss: 0.05473102599, Acc: 25.826%\n",
      "Epoch 120:\n",
      "\tTrain - \tLoss: 0.05363935061, Acc: 30.748%\n",
      "\tEvaluation - \tLoss: 0.05473150458, Acc: 25.792%\n",
      "Epoch 121:\n",
      "\tTrain - \tLoss: 0.05363694809, Acc: 30.765%\n",
      "\tEvaluation - \tLoss: 0.05473198316, Acc: 25.759%\n",
      "Epoch 122:\n",
      "\tTrain - \tLoss: 0.05363456084, Acc: 30.798%\n",
      "\tEvaluation - \tLoss: 0.05473247193, Acc: 25.826%\n",
      "Epoch 123:\n",
      "\tTrain - \tLoss: 0.05363222449, Acc: 30.781%\n",
      "\tEvaluation - \tLoss: 0.05473298107, Acc: 25.826%\n",
      "Epoch 124:\n",
      "\tTrain - \tLoss: 0.0536299136, Acc: 30.79%\n",
      "\tEvaluation - \tLoss: 0.05473346475, Acc: 25.826%\n",
      "Epoch 125:\n",
      "\tTrain - \tLoss: 0.05362763324, Acc: 30.79%\n",
      "\tEvaluation - \tLoss: 0.05473396879, Acc: 25.826%\n",
      "Epoch 126:\n",
      "\tTrain - \tLoss: 0.05362530707, Acc: 30.765%\n",
      "\tEvaluation - \tLoss: 0.05473446774, Acc: 25.826%\n",
      "Epoch 127:\n",
      "\tTrain - \tLoss: 0.05362305726, Acc: 30.84%\n",
      "\tEvaluation - \tLoss: 0.05473497688, Acc: 25.859%\n",
      "Epoch 128:\n",
      "\tTrain - \tLoss: 0.05362080744, Acc: 30.831%\n",
      "\tEvaluation - \tLoss: 0.05473548601, Acc: 25.826%\n",
      "Epoch 129:\n",
      "\tTrain - \tLoss: 0.05361859326, Acc: 30.865%\n",
      "\tEvaluation - \tLoss: 0.05473599006, Acc: 25.826%\n",
      "Epoch 130:\n",
      "\tTrain - \tLoss: 0.05361637398, Acc: 30.873%\n",
      "\tEvaluation - \tLoss: 0.05473650429, Acc: 25.859%\n",
      "Epoch 131:\n",
      "\tTrain - \tLoss: 0.05361419034, Acc: 30.881%\n",
      "\tEvaluation - \tLoss: 0.05473703379, Acc: 25.826%\n",
      "Epoch 132:\n",
      "\tTrain - \tLoss: 0.05361199142, Acc: 30.907%\n",
      "\tEvaluation - \tLoss: 0.05473751747, Acc: 25.859%\n",
      "Epoch 133:\n",
      "\tTrain - \tLoss: 0.05360986377, Acc: 30.94%\n",
      "\tEvaluation - \tLoss: 0.05473805206, Acc: 25.826%\n",
      "Epoch 134:\n",
      "\tTrain - \tLoss: 0.05360773612, Acc: 30.948%\n",
      "\tEvaluation - \tLoss: 0.05473856119, Acc: 25.826%\n",
      "Epoch 135:\n",
      "\tTrain - \tLoss: 0.05360562882, Acc: 30.99%\n",
      "\tEvaluation - \tLoss: 0.05473909579, Acc: 25.792%\n",
      "Epoch 136:\n",
      "\tTrain - \tLoss: 0.05360350626, Acc: 31.015%\n",
      "\tEvaluation - \tLoss: 0.0547396151, Acc: 25.859%\n",
      "Epoch 137:\n",
      "\tTrain - \tLoss: 0.05360144478, Acc: 31.032%\n",
      "\tEvaluation - \tLoss: 0.05474012424, Acc: 25.826%\n",
      "Epoch 138:\n",
      "\tTrain - \tLoss: 0.05359938838, Acc: 31.057%\n",
      "\tEvaluation - \tLoss: 0.05474066392, Acc: 25.826%\n",
      "Epoch 139:\n",
      "\tTrain - \tLoss: 0.05359733199, Acc: 31.057%\n",
      "\tEvaluation - \tLoss: 0.05474118324, Acc: 25.826%\n",
      "Epoch 140:\n",
      "\tTrain - \tLoss: 0.05359531123, Acc: 31.065%\n",
      "\tEvaluation - \tLoss: 0.05474172802, Acc: 25.826%\n",
      "Epoch 141:\n",
      "\tTrain - \tLoss: 0.05359329556, Acc: 31.082%\n",
      "\tEvaluation - \tLoss: 0.05474224224, Acc: 25.759%\n",
      "Epoch 142:\n",
      "\tTrain - \tLoss: 0.05359128498, Acc: 31.073%\n",
      "\tEvaluation - \tLoss: 0.05474278193, Acc: 25.759%\n",
      "Epoch 143:\n",
      "\tTrain - \tLoss: 0.05358929475, Acc: 31.073%\n",
      "\tEvaluation - \tLoss: 0.05474331652, Acc: 25.692%\n",
      "Epoch 144:\n",
      "\tTrain - \tLoss: 0.05358734525, Acc: 31.057%\n",
      "\tEvaluation - \tLoss: 0.05474384602, Acc: 25.726%\n",
      "Epoch 145:\n",
      "\tTrain - \tLoss: 0.05358538048, Acc: 31.09%\n",
      "\tEvaluation - \tLoss: 0.05474438061, Acc: 25.726%\n",
      "Epoch 146:\n",
      "\tTrain - \tLoss: 0.05358342589, Acc: 31.09%\n",
      "\tEvaluation - \tLoss: 0.05474492029, Acc: 25.726%\n",
      "Epoch 147:\n",
      "\tTrain - \tLoss: 0.0535815222, Acc: 31.098%\n",
      "\tEvaluation - \tLoss: 0.05474543452, Acc: 25.759%\n",
      "Epoch 148:\n",
      "\tTrain - \tLoss: 0.05357958797, Acc: 31.115%\n",
      "\tEvaluation - \tLoss: 0.05474598948, Acc: 25.792%\n",
      "Epoch 149:\n",
      "\tTrain - \tLoss: 0.05357768937, Acc: 31.14%\n",
      "\tEvaluation - \tLoss: 0.05474653425, Acc: 25.826%\n",
      "Epoch 150:\n",
      "\tTrain - \tLoss: 0.05357579077, Acc: 31.165%\n",
      "\tEvaluation - \tLoss: 0.05474705866, Acc: 25.859%\n",
      "Epoch 151:\n",
      "\tTrain - \tLoss: 0.05357392271, Acc: 31.182%\n",
      "\tEvaluation - \tLoss: 0.05474759326, Acc: 25.859%\n",
      "Epoch 152:\n",
      "\tTrain - \tLoss: 0.05357208519, Acc: 31.165%\n",
      "\tEvaluation - \tLoss: 0.05474812785, Acc: 25.726%\n",
      "Epoch 153:\n",
      "\tTrain - \tLoss: 0.05357022222, Acc: 31.148%\n",
      "\tEvaluation - \tLoss: 0.05474867771, Acc: 25.726%\n",
      "Epoch 154:\n",
      "\tTrain - \tLoss: 0.05356839997, Acc: 31.148%\n",
      "\tEvaluation - \tLoss: 0.05474922758, Acc: 25.726%\n",
      "Epoch 155:\n",
      "\tTrain - \tLoss: 0.0535665879, Acc: 31.165%\n",
      "\tEvaluation - \tLoss: 0.05474976726, Acc: 25.759%\n",
      "Epoch 156:\n",
      "\tTrain - \tLoss: 0.05356478602, Acc: 31.157%\n",
      "\tEvaluation - \tLoss: 0.05475028149, Acc: 25.792%\n",
      "Epoch 157:\n",
      "\tTrain - \tLoss: 0.05356298922, Acc: 31.182%\n",
      "\tEvaluation - \tLoss: 0.05475082627, Acc: 25.792%\n",
      "Epoch 158:\n",
      "\tTrain - \tLoss: 0.0535612026, Acc: 31.19%\n",
      "\tEvaluation - \tLoss: 0.05475137104, Acc: 25.859%\n",
      "Epoch 159:\n",
      "\tTrain - \tLoss: 0.05355942616, Acc: 31.215%\n",
      "\tEvaluation - \tLoss: 0.05475189036, Acc: 25.859%\n",
      "Epoch 160:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain - \tLoss: 0.05355768026, Acc: 31.223%\n",
      "\tEvaluation - \tLoss: 0.05475245041, Acc: 25.859%\n",
      "Epoch 161:\n",
      "\tTrain - \tLoss: 0.05355594455, Acc: 31.232%\n",
      "\tEvaluation - \tLoss: 0.05475299518, Acc: 25.859%\n",
      "Epoch 162:\n",
      "\tTrain - \tLoss: 0.05355420883, Acc: 31.223%\n",
      "\tEvaluation - \tLoss: 0.05475352977, Acc: 25.859%\n",
      "Epoch 163:\n",
      "\tTrain - \tLoss: 0.05355247311, Acc: 31.198%\n",
      "\tEvaluation - \tLoss: 0.05475407455, Acc: 25.826%\n",
      "Epoch 164:\n",
      "\tTrain - \tLoss: 0.05355074248, Acc: 31.215%\n",
      "\tEvaluation - \tLoss: 0.05475462442, Acc: 25.826%\n",
      "Epoch 165:\n",
      "\tTrain - \tLoss: 0.0535490424, Acc: 31.215%\n",
      "\tEvaluation - \tLoss: 0.05475515901, Acc: 25.792%\n",
      "Epoch 166:\n",
      "\tTrain - \tLoss: 0.05354737285, Acc: 31.223%\n",
      "\tEvaluation - \tLoss: 0.05475569869, Acc: 25.792%\n",
      "Epoch 167:\n",
      "\tTrain - \tLoss: 0.0535457033, Acc: 31.232%\n",
      "\tEvaluation - \tLoss: 0.05475623328, Acc: 25.792%\n",
      "Epoch 168:\n",
      "\tTrain - \tLoss: 0.05354398795, Acc: 31.223%\n",
      "\tEvaluation - \tLoss: 0.05475676788, Acc: 25.792%\n",
      "Epoch 169:\n",
      "\tTrain - \tLoss: 0.05354235403, Acc: 31.207%\n",
      "\tEvaluation - \tLoss: 0.05475731265, Acc: 25.792%\n",
      "Epoch 170:\n",
      "\tTrain - \tLoss: 0.05354069975, Acc: 31.223%\n",
      "\tEvaluation - \tLoss: 0.05475784724, Acc: 25.826%\n",
      "Epoch 171:\n",
      "\tTrain - \tLoss: 0.05353905566, Acc: 31.248%\n",
      "\tEvaluation - \tLoss: 0.05475840729, Acc: 25.826%\n",
      "Epoch 172:\n",
      "\tTrain - \tLoss: 0.05353746246, Acc: 31.265%\n",
      "\tEvaluation - \tLoss: 0.05475893679, Acc: 25.826%\n",
      "Epoch 173:\n",
      "\tTrain - \tLoss: 0.05353581837, Acc: 31.265%\n",
      "\tEvaluation - \tLoss: 0.05475947139, Acc: 25.826%\n",
      "Epoch 174:\n",
      "\tTrain - \tLoss: 0.05353419463, Acc: 31.273%\n",
      "\tEvaluation - \tLoss: 0.05476001107, Acc: 25.826%\n",
      "Epoch 175:\n",
      "\tTrain - \tLoss: 0.05353261671, Acc: 31.273%\n",
      "\tEvaluation - \tLoss: 0.05476055584, Acc: 25.859%\n",
      "Epoch 176:\n",
      "\tTrain - \tLoss: 0.05353098788, Acc: 31.29%\n",
      "\tEvaluation - \tLoss: 0.05476110062, Acc: 25.859%\n",
      "Epoch 177:\n",
      "\tTrain - \tLoss: 0.05352945068, Acc: 31.307%\n",
      "\tEvaluation - \tLoss: 0.0547616403, Acc: 25.893%\n",
      "Epoch 178:\n",
      "\tTrain - \tLoss: 0.05352788802, Acc: 31.298%\n",
      "\tEvaluation - \tLoss: 0.05476215962, Acc: 25.893%\n",
      "Epoch 179:\n",
      "\tTrain - \tLoss: 0.05352628974, Acc: 31.307%\n",
      "\tEvaluation - \tLoss: 0.05476270949, Acc: 25.859%\n",
      "Epoch 180:\n",
      "\tTrain - \tLoss: 0.0535247169, Acc: 31.323%\n",
      "\tEvaluation - \tLoss: 0.05476324408, Acc: 25.859%\n",
      "Epoch 181:\n",
      "\tTrain - \tLoss: 0.05352320006, Acc: 31.34%\n",
      "\tEvaluation - \tLoss: 0.05476378376, Acc: 25.859%\n",
      "Epoch 182:\n",
      "\tTrain - \tLoss: 0.05352166794, Acc: 31.349%\n",
      "\tEvaluation - \tLoss: 0.05476431835, Acc: 25.893%\n",
      "Epoch 183:\n",
      "\tTrain - \tLoss: 0.05352012056, Acc: 31.332%\n",
      "\tEvaluation - \tLoss: 0.05476485295, Acc: 25.859%\n",
      "Epoch 184:\n",
      "\tTrain - \tLoss: 0.05351860372, Acc: 31.323%\n",
      "\tEvaluation - \tLoss: 0.05476538754, Acc: 25.859%\n",
      "Epoch 185:\n",
      "\tTrain - \tLoss: 0.05351711741, Acc: 31.323%\n",
      "\tEvaluation - \tLoss: 0.05476590177, Acc: 25.826%\n",
      "Epoch 186:\n",
      "\tTrain - \tLoss: 0.05351560057, Acc: 31.332%\n",
      "\tEvaluation - \tLoss: 0.05476645672, Acc: 25.826%\n",
      "Epoch 187:\n",
      "\tTrain - \tLoss: 0.05351411935, Acc: 31.349%\n",
      "\tEvaluation - \tLoss: 0.05476697095, Acc: 25.826%\n",
      "Epoch 188:\n",
      "\tTrain - \tLoss: 0.05351260251, Acc: 31.357%\n",
      "\tEvaluation - \tLoss: 0.05476750045, Acc: 25.826%\n",
      "Epoch 189:\n",
      "\tTrain - \tLoss: 0.05351112639, Acc: 31.374%\n",
      "\tEvaluation - \tLoss: 0.05476804523, Acc: 25.792%\n",
      "Epoch 190:\n",
      "\tTrain - \tLoss: 0.05350968589, Acc: 31.365%\n",
      "\tEvaluation - \tLoss: 0.05476856454, Acc: 25.792%\n",
      "Epoch 191:\n",
      "\tTrain - \tLoss: 0.05350816905, Acc: 31.374%\n",
      "\tEvaluation - \tLoss: 0.05476910932, Acc: 25.759%\n",
      "Epoch 192:\n",
      "\tTrain - \tLoss: 0.05350677946, Acc: 31.382%\n",
      "\tEvaluation - \tLoss: 0.05476962864, Acc: 25.759%\n",
      "Epoch 193:\n",
      "\tTrain - \tLoss: 0.05350529824, Acc: 31.382%\n",
      "\tEvaluation - \tLoss: 0.05477015305, Acc: 25.759%\n",
      "Epoch 194:\n",
      "\tTrain - \tLoss: 0.05350383739, Acc: 31.39%\n",
      "\tEvaluation - \tLoss: 0.05477069273, Acc: 25.759%\n",
      "Epoch 195:\n",
      "\tTrain - \tLoss: 0.05350242235, Acc: 31.382%\n",
      "\tEvaluation - \tLoss: 0.05477121714, Acc: 25.759%\n",
      "Epoch 196:\n",
      "\tTrain - \tLoss: 0.05350099204, Acc: 31.39%\n",
      "\tEvaluation - \tLoss: 0.05477173646, Acc: 25.759%\n",
      "Epoch 197:\n",
      "\tTrain - \tLoss: 0.05349958717, Acc: 31.399%\n",
      "\tEvaluation - \tLoss: 0.05477225578, Acc: 25.792%\n",
      "Epoch 198:\n",
      "\tTrain - \tLoss: 0.0534981874, Acc: 31.415%\n",
      "\tEvaluation - \tLoss: 0.05477280055, Acc: 25.826%\n",
      "Epoch 199:\n",
      "\tTrain - \tLoss: 0.05349674182, Acc: 31.44%\n",
      "\tEvaluation - \tLoss: 0.05477331478, Acc: 25.893%\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "for t in range(epochs):\n",
    "    print(\"Epoch \" + str(t) + \":\")\n",
    "    train(model, dataloader_train, optimizer, loss_fn)\n",
    "    test(model, dataloader_test, loss_fn)\n",
    "    torch.save(model.state_dict(), \"model_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f782c65",
   "metadata": {},
   "source": [
    "Here it is possible to save or load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097806e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_40p_train_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19506096",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"model_40p_train_test\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a37216e",
   "metadata": {},
   "source": [
    "Now we plot our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af36d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_array = []\n",
    "\n",
    "fig1, ax1 = plt.subplots(1,1)\n",
    "ax1.semilogx(range(epochs), acc_train_normal_model, label=\"Train\")\n",
    "ax1.semilogx(range(epochs), acc_test_normal_model, label=\"Test\")\n",
    "ax1.set_title(\"Global accuracy\")\n",
    "ax_array.append(ax1)\n",
    "\n",
    "fig2, ax2 = plt.subplots(1,1)\n",
    "ax2.semilogx(range(epochs), loss_train_normal_model, label=\"Train\")\n",
    "ax2.semilogx(range(epochs), loss_test_normal_model, label=\"Test\")\n",
    "ax2.set_title(\"Global loss\")\n",
    "ax_array.append(ax2)\n",
    "\n",
    "\n",
    "fig3, ax3 = plt.subplots(1,1)\n",
    "for i in range(len(genre_acc_train_normal_model)):\n",
    "    ax3.semilogx(range(epochs), genre_acc_train_normal_model[i], label=lookup_dict_reverse[i])\n",
    "\n",
    "fig4, ax4 = plt.subplots(1,1)\n",
    "for i in range(len(genre_acc_test_normal_model)):\n",
    "    ax4.semilogx(range(epochs), genre_acc_test_normal_model[i], label=lookup_dict_reverse[i])\n",
    "\n",
    "ax3.set_title(\"Train Genre Accuracy\")\n",
    "ax_array.append(ax3)\n",
    "\n",
    "ax4.set_title(\"Test Genre Accuracy\")\n",
    "ax_array.append(ax4)\n",
    "\n",
    "for ax_p in ax_array:\n",
    "    box = ax_p.get_position()\n",
    "    ax_p.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9])\n",
    "    ax_p.legend(loc='upper center', bbox_to_anchor=(0.5, -0.08), fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "if not os.path.isdir(\"plots\"): os.mkdir(\"plots\")\n",
    "fig1.savefig(\"plots/global_accuracy.png\")\n",
    "fig2.savefig(\"plots/global_loss.png\")\n",
    "fig3.savefig(\"plots/train_genre_accuracy.png\")\n",
    "fig4.savefig(\"plots/test_genre_accuracy.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
