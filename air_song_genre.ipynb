{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545f880a",
   "metadata": {},
   "source": [
    "First, we manage all imports for the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a87ebc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder you are executing pip from can no longer be found.\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85666458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import ast\n",
    "from torch import nn\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "from torch.utils.data import random_split, Subset, DataLoader\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7c527",
   "metadata": {},
   "source": [
    "Next we process the zipped data to csv. It is stored in .zip, because it is smaller on git and then locally it is extracted to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edba73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"data/dataset.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")\n",
    "\n",
    "for file_name in os.listdir(\"data/dataset\"):\n",
    "    source = \"data/dataset/\" + file_name\n",
    "    destination = \"data/\" + file_name\n",
    "    if os.path.isfile(source): shutil.move(source, destination)\n",
    "\n",
    "os.rmdir(\"data/dataset\")\n",
    "del source, destination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3a4fc",
   "metadata": {},
   "source": [
    "Preprocessing of the datasets, the goal is to receive a table with the following columns: Name, Lyrics, Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b36467",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('data/Spotify-2000.csv')\n",
    "df = df[['Title', 'Top Genre']] #take only the name and genre\n",
    "df2 = pd.read_csv('data/spotify_millsongdata.csv')\n",
    "df['lyrics'] = ' ' # add column lyrics\n",
    "#Now combine those two documents by the title\n",
    "found = 0\n",
    "\n",
    "for x, title in enumerate(df['Title']):\n",
    "    #print(title)\n",
    "    for y, title2 in enumerate(df2['song']):\n",
    "        if title2.lower() == title.lower():\n",
    "            df['lyrics'].iloc[x] = df2['text'].iloc[y]\n",
    "\n",
    "print(\"finished combining..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98621f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove songs which were not in both datsets\n",
    "songs_to_remove = []\n",
    "for x, lyrics in enumerate(df['lyrics']):\n",
    "    if lyrics == '':\n",
    "        songs_to_remove.append(x)\n",
    "df.drop(songs_to_remove, axis = 0, inplace = True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e76e7",
   "metadata": {},
   "source": [
    "Further preprocessing of the lyrics itself. We remove the stopwords and punctuations with regex and stopwords from ntlk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db225637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "punc_regex = r'[^\\s\\w]' #searches for everything thats not a word or space\n",
    "stopword_regex = r'\\b{0}\\b'\n",
    "\n",
    "print(df['lyrics'].iloc[1])\n",
    "\n",
    "for x, lyrics in enumerate(df['lyrics']):\n",
    "    txt = lyrics\n",
    "    txt = re.sub(punc_regex, '', txt) #remove punuctuations\n",
    "    for sword in sw:\n",
    "        txt = re.sub(stopword_regex.format(sword), '', txt, flags=re.IGNORECASE) #remove every stopword\n",
    "    df['lyrics'].iloc[x] = txt\n",
    "\n",
    "#print(df)\n",
    "print(df['lyrics'].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded6eae",
   "metadata": {},
   "source": [
    "Tokenize the lyrics and create tagged Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f23826",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = []\n",
    "\n",
    "for i,d in enumerate(df['lyrics']):\n",
    "    tokenized_words = nltk.tokenize.word_tokenize(d)\n",
    "    tagged_data.append(TaggedDocument(words=tokenized_words, tags=str(i)))\n",
    "\n",
    "print(tagged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982a54b",
   "metadata": {},
   "source": [
    "Setting up the Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e8276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=1, epochs=30)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "print(\"Doc2Vec model finished training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0446769",
   "metadata": {},
   "source": [
    "Building the DataLoader for the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5778176",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Token'] + ['Target']\n",
    "df_for_dataloader = pd.DataFrame(columns = columns)\n",
    "df_for_dataloader.set_index(columns)\n",
    "\n",
    "for i in range(len(tagged_data)):\n",
    "    token = [model.infer_vector(tagged_data[i][0])]\n",
    "    #token = [list(model.dv[i])]\n",
    "    target = [5] # should be genre\n",
    "    row = pd.DataFrame([token + target], columns=['Token', 'Target'])\n",
    "    df_for_dataloader = pd.concat([df_for_dataloader, row])\n",
    "    \n",
    "display(df_for_dataloader.iloc[:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b771ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu') #torch.device('cuda' if torch.cuda.is_available() else 'cpu') For faster training in the end\n",
    "\n",
    "class SongModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SongModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = input_size * 3\n",
    "        self.output_size = output_size\n",
    "        self.layer_1 =   nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.layer_2 =   nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.layer_out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax =   nn.Softmax()\n",
    "        self.sigmoid =   nn.Sigmoid()\n",
    "        self.tanh =      nn.Tanh()\n",
    "\n",
    "        nn.init.xavier_uniform_(self.layer_1.weight)\n",
    "        nn.init.zeros_(self.layer_1.bias)\n",
    "        nn.init.xavier_uniform_(self.layer_2.weight)\n",
    "        nn.init.zeros_(self.layer_2.bias)\n",
    "        nn.init.xavier_uniform_(self.layer_out.weight)\n",
    "        nn.init.zeros_(self.layer_out.bias)\n",
    "\n",
    "\n",
    "    def forward(self, d):\n",
    "        x = self.sigmoid(self.layer_1(d))\n",
    "        x = self.tanh(self.layer_2(x))\n",
    "        x = self.softmax(self.layer_out(x))\n",
    "        return x\n",
    "\n",
    "model = SongModel(1, 3).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.025)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c39610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    total_loss = 0\n",
    "    iterations = 0\n",
    "    for sample in dataloader:\n",
    "        model_input = sample[0]\n",
    "        should = sample[1]\n",
    "        predict = model(model_input)\n",
    "        loss = loss_fn(predict, should.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        iterations += 1\n",
    "        if iterations % 1000 == 0:\n",
    "            print(f\"Predict: {predict}, Loss: {loss}\")\n",
    "\n",
    "    print(total_loss / len(dataloader))\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
