{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545f880a",
   "metadata": {},
   "source": [
    "First, we manage all imports for the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ebc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install install -Uq bertopic\n",
    "!pip install torch\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85666458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import ast\n",
    "from torch import nn\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from torch.utils.data import random_split, Subset, DataLoader\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import bertopic\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7c527",
   "metadata": {},
   "source": [
    "Next we process the zipped data to csv. It is stored in .zip, because it is smaller on git and then locally it is extracted to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edba73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"data/dataset.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")\n",
    "\n",
    "for file_name in os.listdir(\"data/dataset\"):\n",
    "    source = \"data/dataset/\" + file_name\n",
    "    destination = \"data/\" + file_name\n",
    "    if os.path.isfile(source): shutil.move(source, destination)\n",
    "\n",
    "os.rmdir(\"data/dataset\")\n",
    "del source, destination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3a4fc",
   "metadata": {},
   "source": [
    "Preprocessing of the datasets, the goal is to receive a table with the following columns: Name, Lyrics, Genre<br>\n",
    "df Dataset - Name, Lyrics, Genre<br>\n",
    "df3 Dataset - 10000 entrys of random selectet lyrics to train doc2vec\n",
    "df4 Dataset - 28k~ entrys of songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b36467",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('data/Spotify-2000.csv')\n",
    "df = df[['Title', 'Top Genre']] #take only the name and genre\n",
    "df2 = pd.read_csv('data/spotify_millsongdata.csv')\n",
    "df4 = pd.read_csv('data/tcc_ceds_music.csv')\n",
    "df4 = df4[['track_name','genre','lyrics']]\n",
    "\n",
    "df['lyrics'] = '' # add column lyrics\n",
    "#Now combine those two documents by the title\n",
    "found = 0\n",
    "\n",
    "for x, title in enumerate(df['Title']):\n",
    "    #print(title)\n",
    "    for y, title2 in enumerate(df2['song']):\n",
    "        if title2.lower() == title.lower():\n",
    "            df['lyrics'].iloc[x] = df2['text'].iloc[y]\n",
    "\n",
    "print(\"finished combining..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e1a368",
   "metadata": {},
   "source": [
    "Collect 40000 random entrys of lyrics from the millsongdata Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2['text'].sample(n=40000)\n",
    "#print(df3)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98621f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove songs which were not in both datsets\n",
    "songs_to_remove = []\n",
    "for x, lyrics in enumerate(df['lyrics']):\n",
    "    if lyrics == '':\n",
    "        songs_to_remove.append(x)\n",
    "df.drop(songs_to_remove, axis = 0, inplace = True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e76e7",
   "metadata": {},
   "source": [
    "Further preprocessing of the lyrics itself. We remove the stopwords and punctuations with regex and stopwords from ntlk form df and df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db225637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "sw = stopwords.words('english')\n",
    "punc_regex = r'[^\\s\\w]' #searches for everything thats not a word or space\n",
    "stopword_regex = r'\\b{0}\\b'\n",
    "space_regex = r'\\s\\s+'\n",
    "newl_regex = r'\\n|\\r'\n",
    "\n",
    "#print(df3.iloc[1])\n",
    "\n",
    "#preprocessing of df\n",
    "for x, lyrics in enumerate(df['lyrics']):\n",
    "    txt = lyrics\n",
    "    txt = re.sub(punc_regex, '', txt) #remove punuctuations\n",
    "    for sword in sw:\n",
    "        txt = re.sub(stopword_regex.format(sword), '', txt, flags=re.IGNORECASE) #remove every stopword\n",
    "    txt = re.sub(newl_regex, '', txt)\n",
    "    stemmed_txt = ''\n",
    "    for word in txt.split():\n",
    "        stemmed_txt += str(stemmer.stem(word.lower())) + \" \"\n",
    "    \n",
    "    df['lyrics'].iloc[x] = stemmed_txt\n",
    "\n",
    "print(df)\n",
    "print('finished preprocessing of df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing of df3 -> dataset for doc2vec training\n",
    "\n",
    "preprocess = False\n",
    "\n",
    "if(preprocess):\n",
    "    for x, lyrics in enumerate(df3):\n",
    "        txt = lyrics\n",
    "        txt = re.sub(punc_regex, '', txt) #remove punuctuations\n",
    "        for sword in sw:\n",
    "            txt = re.sub(stopword_regex.format(sword), '', txt, flags=re.IGNORECASE) #remove every stopword\n",
    "        txt = re.sub(newl_regex, '', txt)\n",
    "        stemmed_txt = ''\n",
    "        for word in txt.split():\n",
    "            stemmed_txt += str(stemmer.stem(word.lower())) + \" \"\n",
    "\n",
    "        df3.iloc[x] = stemmed_txt\n",
    "    os.remove('data/doc2vec_training_data.csv')\n",
    "    df3.to_csv('data/doc2vec_training_data.csv', index=False)\n",
    "else:\n",
    "    df3 = pd.read_csv('data/doc2vec_training_data.csv')\n",
    "\n",
    "print(df3)\n",
    "print('finished preprocessing of df3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a0530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing of df4\n",
    "for x, lyrics in enumerate(df4['lyrics']):\n",
    "    txt = lyrics\n",
    "    txt = re.sub(punc_regex, '', txt) #remove punuctuations\n",
    "    for sword in sw:\n",
    "        txt = re.sub(stopword_regex.format(sword), '', txt, flags=re.IGNORECASE) #remove every stopword\n",
    "    \n",
    "    txt = re.sub(newl_regex, '', txt)\n",
    "    stemmed_txt = ''\n",
    "    for word in txt.split():\n",
    "        stemmed_txt += str(stemmer.stem(word.lower())) + \" \"\n",
    "    df4['lyrics'].iloc[x] = stemmed_txt\n",
    "\n",
    "print(df4)\n",
    "print('finished preprocessing of df4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43056ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "#display(df)\n",
    "\n",
    "topic_model = bertopic.BERTopic(language='english', embedding_model=sentence_model,verbose=True)\n",
    "\n",
    "topics, probs = topic_model.fit_transform(df[\"lyrics\"])\n",
    "\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad17c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_model.get_topic_info())\n",
    "\n",
    "print(topic_model.get_topics()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded6eae",
   "metadata": {},
   "source": [
    "Tokenize the lyrics and create tagged Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f23826",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = []\n",
    "nltk.download('punkt')\n",
    "\n",
    "for i,d in enumerate(df3):\n",
    "    tokenized_words = nltk.tokenize.word_tokenize(d)\n",
    "    tagged_data.append(TaggedDocument(words=tokenized_words, tags=str(i)))\n",
    "\n",
    "print(len(tagged_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982a54b",
   "metadata": {},
   "source": [
    "Setting up the Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e8276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=1, epochs=30)\n",
    "doc2vec_model.build_vocab(tagged_data)\n",
    "doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n",
    "print(\"Doc2Vec model finished training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0446769",
   "metadata": {},
   "source": [
    "Building the DataLoader for the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5778176",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Token'] + ['Target']\n",
    "df_for_dataloader = pd.DataFrame(columns = columns)\n",
    "df_for_dataloader.set_index(columns)\n",
    "\n",
    "lookup_dict = {}\n",
    "token_list = []\n",
    "target_list = []\n",
    "i = 0\n",
    "for d in df4[\"genre\"]:\n",
    "    if d not in lookup_dict:\n",
    "        lookup_dict[d] = i\n",
    "        i+=1\n",
    "\n",
    "for index, df_row in df4.iterrows():\n",
    "    lyrics_tokenized = nltk.tokenize.word_tokenize(df_row[\"lyrics\"])\n",
    "    token = [doc2vec_model.infer_vector(lyrics_tokenized)]\n",
    "    #print(token)\n",
    "    one_hot_encoded_vector = []\n",
    "    for x in lookup_dict.keys():\n",
    "        if df_row[\"genre\"] == x:\n",
    "            one_hot_encoded_vector.append(1)\n",
    "        else:\n",
    "            one_hot_encoded_vector.append(0)\n",
    "    target = [np.array(one_hot_encoded_vector)] # should be genre\n",
    "    #row = pd.DataFrame([token + target], columns=['Token', 'Target'])\n",
    "    #df_for_dataloader = pd.concat([df_for_dataloader, row])\n",
    "    token_list.append(token)\n",
    "    target_list.append(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8b7b4",
   "metadata": {},
   "source": [
    "Splitting the dataset into train and test (80,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8197733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = torch.utils.data.random_split(list(zip(token_list, target_list)), [0.8, 0.2])\n",
    "print(len(data_train))\n",
    "print(len(data_test))\n",
    "    \n",
    "dataloader_train = torch.utils.data.DataLoader(data_train, batch_size=32)\n",
    "dataloader_test = torch.utils.data.DataLoader(data_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0da18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_test_count_dict = {}\n",
    "for i in data_test:\n",
    "  if np.argmax(i[1]) in genre_test_count_dict:\n",
    "    genre_test_count_dict[np.argmax(i[1])] += 1\n",
    "  else:\n",
    "    genre_test_count_dict[np.argmax(i[1])] = 1\n",
    "\n",
    "genre_train_count_dict = {}\n",
    "for i in data_train:\n",
    "  if np.argmax(i[1]) in genre_train_count_dict:\n",
    "    genre_train_count_dict[np.argmax(i[1])] += 1\n",
    "  else:\n",
    "    genre_train_count_dict[np.argmax(i[1])] = 1\n",
    "\n",
    "for i in genre_test_count_dict:\n",
    "  print(i, genre_test_count_dict[i])\n",
    "\n",
    "lookup_dict_reverse = {y: x for x, y in lookup_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b01396",
   "metadata": {},
   "source": [
    "We define out neurol network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b771ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu') #torch.device('cuda' if torch.cuda.is_available() else 'cpu') For faster training in the end\n",
    "\n",
    "class SongModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SongModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layer_1 =   nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.layer_2 =   nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.layer_out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax =   nn.Softmax()\n",
    "        self.sigmoid =   nn.Sigmoid()\n",
    "        self.tanh =      nn.Tanh()\n",
    "\n",
    "        nn.init.xavier_uniform_(self.layer_1.weight)\n",
    "        nn.init.zeros_(self.layer_1.bias)\n",
    "        nn.init.xavier_uniform_(self.layer_out.weight)\n",
    "        nn.init.zeros_(self.layer_out.bias)\n",
    "\n",
    "\n",
    "    def forward(self, d):\n",
    "        x = self.sigmoid(self.layer_1(d))\n",
    "        x = self.softmax(self.layer_out(x))\n",
    "        return x\n",
    "\n",
    "model = SongModel(dataloader_train.dataset[0][0][0].size, dataloader_train.dataset[0][1][0].size).to(device)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e2246b",
   "metadata": {},
   "source": [
    "Define lists to save the accuracy and loss in each iteration to plot them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbc0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_normal_model = []\n",
    "acc_test_normal_model = []\n",
    "loss_train_normal_model = []\n",
    "loss_test_normal_model = []\n",
    "genre_acc_train_normal_model = []\n",
    "genre_acc_test_normal_model = []\n",
    "for i in range(len(lookup_dict)):\n",
    "    genre_acc_train_normal_model.append([])\n",
    "    genre_acc_test_normal_model.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5badb",
   "metadata": {},
   "source": [
    "Here is the training and test function for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d742ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer,loss_fn):\n",
    "    total_loss = 0\n",
    "    acc = 0\n",
    "    genre_accuracy = {}\n",
    "    for x, key in enumerate(lookup_dict):\n",
    "        genre_accuracy[x] = 0\n",
    "\n",
    "    for sample in dataloader:\n",
    "        model_input = sample[0][0]\n",
    "        should = sample[1][0]\n",
    "        predict = model(model_input)\n",
    "        for sample_index in range(len(should)):\n",
    "            predict_e = predict[sample_index].data.numpy()\n",
    "            should_e = should[sample_index].numpy()\n",
    "            if np.where(predict_e == np.amax(predict_e))[0][0] == np.where(should_e == np.amax(should_e))[0][0]:\n",
    "                acc += 1\n",
    "                genre_accuracy[np.where(should_e == np.amax(should_e))[0][0]] += 1\n",
    "        loss = loss_fn(predict, should.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "    loss_train_normal_model.append(total_loss.detach().numpy() / len(dataloader.dataset))\n",
    "    acc_train_normal_model.append(acc * 100 / len(dataloader.dataset))\n",
    "    print(f\"\\tTrain - \\tLoss: {loss_train_normal_model[-1]:3.10}, Acc: {acc_train_normal_model[-1]:3.5}%\", end=\"\\n\")\n",
    "    for (genre, id_genre) in lookup_dict.items():\n",
    "        genre_acc_train_normal_model[id_genre].append(round(genre_accuracy[lookup_dict[genre]] * 100 / genre_train_count_dict[id_genre],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, loss_fn):\n",
    "    total_loss = 0\n",
    "    acc = 0\n",
    "    genre_accuracy = {}\n",
    "    for x, key in enumerate(lookup_dict):\n",
    "        genre_accuracy[x] = 0\n",
    "\n",
    "    for sample in dataloader:\n",
    "        model_input = sample[0][0]\n",
    "        should = sample[1][0]\n",
    "        predict = model(model_input)\n",
    "        for sample_index in range(len(should)):\n",
    "            predict_e = predict[sample_index].data.numpy()\n",
    "            should_e = should[sample_index].numpy()\n",
    "            if np.where(predict_e == np.amax(predict_e))[0][0] == np.where(should_e == np.amax(should_e))[0][0]:\n",
    "                acc += 1\n",
    "                genre_accuracy[np.where(should_e == np.amax(should_e))[0][0]] += 1\n",
    "\n",
    "        loss = loss_fn(predict, should.float())\n",
    "        total_loss += loss\n",
    "\n",
    "    loss_test_normal_model.append(total_loss.detach().numpy() / len(dataloader.dataset))\n",
    "    acc_test_normal_model.append(acc * 100 / len(dataloader.dataset))\n",
    "    print(f\"\\tEvaluation - \\tLoss: {loss_test_normal_model[-1]:3.10}, Acc: {acc_test_normal_model[-1]:3.5}%\", end=\"\\n\")\n",
    "    for (genre, id_genre) in lookup_dict.items():\n",
    "        genre_acc_test_normal_model[id_genre].append(round(genre_accuracy[lookup_dict[genre]] * 100 / genre_test_count_dict[id_genre],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ebde06",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for t in range(epochs):\n",
    "    print(\"Epoch \" + str(t) + \":\")\n",
    "    train(model, dataloader_train, optimizer, loss_fn)\n",
    "    test(model, dataloader_test, loss_fn)\n",
    "    torch.save(model.state_dict(), \"model_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d481d4",
   "metadata": {},
   "source": [
    "Here it is possible to save or load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097806e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_40p_train_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af930980",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"model_40p_train_test\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdff685",
   "metadata": {},
   "source": [
    "Now we plot our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af36d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_array = []\n",
    "\n",
    "fig1, ax1 = plt.subplots(1,1)\n",
    "ax1.semilogx(range(epochs), acc_train_normal_model, label=\"Train\")\n",
    "ax1.semilogx(range(epochs), acc_test_normal_model, label=\"Test\")\n",
    "ax1.set_title(\"Global accuracy\")\n",
    "ax_array.append(ax1)\n",
    "\n",
    "fig2, ax2 = plt.subplots(1,1)\n",
    "ax2.semilogx(range(epochs), loss_train_normal_model, label=\"Train\")\n",
    "ax2.semilogx(range(epochs), loss_test_normal_model, label=\"Test\")\n",
    "ax2.set_title(\"Global loss\")\n",
    "ax_array.append(ax2)\n",
    "\n",
    "\n",
    "fig3, ax3 = plt.subplots(1,1)\n",
    "for i in range(len(genre_acc_train_normal_model)):\n",
    "    ax3.semilogx(range(epochs), genre_acc_train_normal_model[i], label=lookup_dict_reverse[i])\n",
    "\n",
    "fig4, ax4 = plt.subplots(1,1)\n",
    "for i in range(len(genre_acc_test_normal_model)):\n",
    "    ax4.semilogx(range(epochs), genre_acc_test_normal_model[i], label=lookup_dict_reverse[i])\n",
    "\n",
    "ax3.set_title(\"Train Genre Accuracy\")\n",
    "ax_array.append(ax3)\n",
    "\n",
    "ax4.set_title(\"Test Genre Accuracy\")\n",
    "ax_array.append(ax4)\n",
    "\n",
    "for ax_p in ax_array:\n",
    "    box = ax_p.get_position()\n",
    "    ax_p.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9])\n",
    "    ax_p.legend(loc='upper center', bbox_to_anchor=(0.5, -0.08), fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "if not os.path.isdir(\"plots\"): os.mkdir(\"plots\")\n",
    "fig1.savefig(\"plots/global_accuracy.png\")\n",
    "fig2.savefig(\"plots/global_loss.png\")\n",
    "fig3.savefig(\"plots/train_genre_accuracy.png\")\n",
    "fig4.savefig(\"plots/test_genre_accuracy.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
