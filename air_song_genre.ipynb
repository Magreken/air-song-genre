{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545f880a",
   "metadata": {},
   "source": [
    "First, we manage all imports for the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ebc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install install -Uq bertopic\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85666458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import ast\n",
    "from torch import nn\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "from torch.utils.data import random_split, Subset, DataLoader\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import bertopic\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7c527",
   "metadata": {},
   "source": [
    "Next we process the zipped data to csv. It is stored in .zip, because it is smaller on git and then locally it is extracted to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edba73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"data/dataset.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")\n",
    "\n",
    "for file_name in os.listdir(\"data/dataset\"):\n",
    "    source = \"data/dataset/\" + file_name\n",
    "    destination = \"data/\" + file_name\n",
    "    if os.path.isfile(source): shutil.move(source, destination)\n",
    "\n",
    "os.rmdir(\"data/dataset\")\n",
    "del source, destination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3a4fc",
   "metadata": {},
   "source": [
    "Preprocessing of the datasets, the goal is to receive a table with the following columns: Name, Lyrics, Genre<br>\n",
    "df Dataset - Name, Lyrics, Genre<br>\n",
    "df3 Dataset - 10000 entrys of random selectet lyrics to train doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b36467",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('data/Spotify-2000.csv')\n",
    "df = df[['Title', 'Top Genre']] #take only the name and genre\n",
    "df2 = pd.read_csv('data/spotify_millsongdata.csv')\n",
    "df4 = pd.read_csv('data/tcc_ceds_music.csv')\n",
    "df4 = df4[['track_name','genre','lyrics']]\n",
    "\n",
    "df['lyrics'] = '' # add column lyrics\n",
    "#Now combine those two documents by the title\n",
    "found = 0\n",
    "\n",
    "for x, title in enumerate(df['Title']):\n",
    "    #print(title)\n",
    "    for y, title2 in enumerate(df2['song']):\n",
    "        if title2.lower() == title.lower():\n",
    "            df['lyrics'].iloc[x] = df2['text'].iloc[y]\n",
    "\n",
    "print(\"finished combining..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e1a368",
   "metadata": {},
   "source": [
    "Collect 10000 random entrys of lyrics from the millsongdata Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2['text'].sample(n=10000)\n",
    "print(df3)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98621f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove songs which were not in both datsets\n",
    "songs_to_remove = []\n",
    "for x, lyrics in enumerate(df['lyrics']):\n",
    "    if lyrics == '':\n",
    "        songs_to_remove.append(x)\n",
    "df.drop(songs_to_remove, axis = 0, inplace = True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e76e7",
   "metadata": {},
   "source": [
    "Further preprocessing of the lyrics itself. We remove the stopwords and punctuations with regex and stopwords from ntlk form df and df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db225637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "punc_regex = r'[^\\s\\w]' #searches for everything thats not a word or space\n",
    "stopword_regex = r'\\b{0}\\b'\n",
    "space_regex = r'\\s\\s+'\n",
    "newl_regex = r'\\n|\\r'\n",
    "\n",
    "print(df3.iloc[1])\n",
    "\n",
    "#preprocessing of df\n",
    "for x, lyrics in enumerate(df['lyrics']):\n",
    "    txt = lyrics\n",
    "    txt = re.sub(punc_regex, '', txt) #remove punuctuations\n",
    "    for sword in sw:\n",
    "        txt = re.sub(stopword_regex.format(sword), '', txt, flags=re.IGNORECASE) #remove every stopword\n",
    "    txt = re.sub(space_regex, ' ', txt)\n",
    "    df['lyrics'].iloc[x] = txt\n",
    "\n",
    "\n",
    "    \n",
    "#preprocessing of df3\n",
    "for x, lyrics in enumerate(df3):\n",
    "    txt = lyrics\n",
    "    txt = re.sub(punc_regex, '', txt) #remove punuctuations\n",
    "    for sword in sw:\n",
    "        txt = re.sub(stopword_regex.format(sword), '', txt, flags=re.IGNORECASE) #remove every stopword\n",
    "    txt = re.sub(space_regex, ' ', txt)\n",
    "    df3.iloc[x] = txt\n",
    "    \n",
    "#print(df)\n",
    "print(df3.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a0530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing of df4\n",
    "for x, lyrics in enumerate(df4['lyrics']):\n",
    "    txt = lyrics\n",
    "    txt = re.sub(punc_regex, '', txt) #remove punuctuations\n",
    "    for sword in sw:\n",
    "        txt = re.sub(stopword_regex.format(sword), '', txt, flags=re.IGNORECASE) #remove every stopword\n",
    "    txt = re.sub(space_regex, ' ', txt)\n",
    "    df4['lyrics'].iloc[x] = txt\n",
    "    \n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43056ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "#display(df)\n",
    "\n",
    "topic_model = bertopic.BERTopic(language='english', embedding_model=sentence_model,verbose=True)\n",
    "\n",
    "topics, probs = topic_model.fit_transform(df[\"lyrics\"])\n",
    "\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad17c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_model.get_topic_info())\n",
    "\n",
    "print(topic_model.get_topics()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded6eae",
   "metadata": {},
   "source": [
    "Tokenize the lyrics and create tagged Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f23826",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = []\n",
    "nltk.download('punkt')\n",
    "\n",
    "for i,d in enumerate(df4['lyrics']):\n",
    "    tokenized_words = nltk.tokenize.word_tokenize(d)\n",
    "    tagged_data.append(TaggedDocument(words=tokenized_words, tags=str(i)))\n",
    "\n",
    "print(len(tagged_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982a54b",
   "metadata": {},
   "source": [
    "Setting up the Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e8276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=1, epochs=30)\n",
    "doc2vec_model.build_vocab(tagged_data)\n",
    "doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n",
    "print(\"Doc2Vec model finished training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0446769",
   "metadata": {},
   "source": [
    "Building the DataLoader for the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5778176",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Token'] + ['Target']\n",
    "df_for_dataloader = pd.DataFrame(columns = columns)\n",
    "df_for_dataloader.set_index(columns)\n",
    "\n",
    "lookup_dict = {}\n",
    "token_list = []\n",
    "target_list = []\n",
    "i = 0\n",
    "for d in df4[\"genre\"]:\n",
    "    if d not in lookup_dict:\n",
    "        lookup_dict[d] = i\n",
    "        i+=1\n",
    "\n",
    "for index, df_row in df4.iterrows():\n",
    "    lyrics_tokenized = nltk.tokenize.word_tokenize(df_row[\"lyrics\"])\n",
    "    token = [doc2vec_model.infer_vector(lyrics_tokenized)]\n",
    "    #print(token)\n",
    "    one_hot_encoded_vector = []\n",
    "    for x in lookup_dict.keys():\n",
    "        if df_row[\"genre\"] == x:\n",
    "            one_hot_encoded_vector.append(1)\n",
    "        else:\n",
    "            one_hot_encoded_vector.append(0)\n",
    "    target = [np.array(one_hot_encoded_vector)] # should be genre\n",
    "    #row = pd.DataFrame([token + target], columns=['Token', 'Target'])\n",
    "    #df_for_dataloader = pd.concat([df_for_dataloader, row])\n",
    "    token_list.append(token)\n",
    "    target_list.append(target)\n",
    "\n",
    "    \n",
    "dataloader = torch.utils.data.DataLoader(list(zip(token_list,target_list)), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b771ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu') #torch.device('cuda' if torch.cuda.is_available() else 'cpu') For faster training in the end\n",
    "\n",
    "class SongModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SongModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layer_1 =   nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.layer_2 =   nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.layer_out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax =   nn.Softmax()\n",
    "        self.sigmoid =   nn.Sigmoid()\n",
    "        self.tanh =      nn.Tanh()\n",
    "\n",
    "        nn.init.xavier_uniform_(self.layer_1.weight)\n",
    "        nn.init.zeros_(self.layer_1.bias)\n",
    "        nn.init.xavier_uniform_(self.layer_2.weight)\n",
    "        nn.init.zeros_(self.layer_2.bias)\n",
    "        nn.init.xavier_uniform_(self.layer_out.weight)\n",
    "        nn.init.zeros_(self.layer_out.bias)\n",
    "\n",
    "\n",
    "    def forward(self, d):\n",
    "        x = self.sigmoid(self.layer_1(d))\n",
    "        # x = self.tanh(self.layer_2(x))\n",
    "        x = self.softmax(self.layer_out(x))\n",
    "        return x\n",
    "\n",
    "model = SongModel(dataloader.dataset[0][0][0].size, dataloader.dataset[0][1][0].size).to(device)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d742ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "for t in range(epochs):\n",
    "    total_loss = 0\n",
    "    iterations = 0\n",
    "    acc = 0\n",
    "    count = 0\n",
    "    for sample in dataloader:\n",
    "        model_input = sample[0][0]\n",
    "        should = sample[1][0]\n",
    "        predict = model(model_input)\n",
    "        for sample_index in range(len(sample)):\n",
    "            count += 1\n",
    "            predict_e = predict[sample_index].data.numpy()\n",
    "            should_e = should[sample_index].numpy()\n",
    "            if np.where(predict_e == np.amax(predict_e))[0][0] == np.where(should_e == np.amax(should_e))[0][0]:\n",
    "                acc += 1\n",
    "        loss = loss_fn(predict, should.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        iterations += 1\n",
    "    # if t % 10 == 0:\n",
    "    print(f\"Epoch {t+1:3}. Loss: {total_loss / len(dataloader):3.10}, Acc: {acc *100 / count:3.5}%\", end=\"\\n\")\n",
    "torch.save(model.state_dict(), \"model_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_xxp\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
