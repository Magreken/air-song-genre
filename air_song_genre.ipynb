{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545f880a",
   "metadata": {},
   "source": [
    "First, we manage all imports for the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a87ebc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.9.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: pyfume in /usr/local/lib/python3.10/dist-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: pandas in /home/alfrodo/.local/lib/python3.10/site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.1)\n",
      "Requirement already satisfied: fst-pso in /usr/local/lib/python3.10/dist-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: simpful in /usr/local/lib/python3.10/dist-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in /usr/local/lib/python3.10/dist-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/alfrodo/.local/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/alfrodo/.local/lib/python3.10/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in /home/alfrodo/.local/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/alfrodo/.local/lib/python3.10/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /home/alfrodo/.local/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.6.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install install -Uq bertopic\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85666458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alfrodo/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import ast\n",
    "from torch import nn\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from torch.utils.data import random_split, Subset, DataLoader\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import bertopic\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7c527",
   "metadata": {},
   "source": [
    "Next we process the zipped data to csv. It is stored in .zip, because it is smaller on git and then locally it is extracted to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2edba73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"data/dataset.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")\n",
    "\n",
    "for file_name in os.listdir(\"data/dataset\"):\n",
    "    source = \"data/dataset/\" + file_name\n",
    "    destination = \"data/\" + file_name\n",
    "    if os.path.isfile(source): shutil.move(source, destination)\n",
    "\n",
    "os.rmdir(\"data/dataset\")\n",
    "del source, destination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3a4fc",
   "metadata": {},
   "source": [
    "Preprocessing of the datasets, the goal is to receive a table with the following columns: Name, Lyrics, Genre<br>\n",
    "df Dataset - Name, Lyrics, Genre<br>\n",
    "df3 Dataset - 10000 entrys of random selectet lyrics to train doc2vec\n",
    "df4 Dataset - 28k~ entrys of songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b36467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished combining..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('data/Spotify-2000.csv')\n",
    "df = df[['Title', 'Top Genre']] #take only the name and genre\n",
    "df2 = pd.read_csv('data/spotify_millsongdata.csv')\n",
    "df4 = pd.read_csv('data/tcc_ceds_music.csv')\n",
    "df4 = df4[['track_name','genre','lyrics']]\n",
    "\n",
    "df['lyrics'] = '' # add column lyrics\n",
    "#Now combine those two documents by the title\n",
    "found = 0\n",
    "\n",
    "for x, title in enumerate(df['Title']):\n",
    "    #print(title)\n",
    "    for y, title2 in enumerate(df2['song']):\n",
    "        if title2.lower() == title.lower():\n",
    "            df['lyrics'].iloc[x] = df2['text'].iloc[y]\n",
    "\n",
    "print(\"finished combining..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e1a368",
   "metadata": {},
   "source": [
    "Collect 10000 random entrys of lyrics from the millsongdata Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40cf936b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "df3 = df2['text'].sample(n=10000)\n",
    "#print(df3)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a98621f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Title          Top Genre  \\\n",
      "0               Sunrise    adult standards   \n",
      "1           Black Night         album rock   \n",
      "3         The Pretender  alternative metal   \n",
      "6     She Will Be Loved                pop   \n",
      "8        Mr. Brightside        modern rock   \n",
      "...                 ...                ...   \n",
      "1988         Summertime    adult standards   \n",
      "1989   Heartbreak Hotel    adult standards   \n",
      "1990          Hound Dog    adult standards   \n",
      "1991    Johnny B. Goode         blues rock   \n",
      "1993     Blueberry Hill    adult standards   \n",
      "\n",
      "                                                 lyrics  \n",
      "0     You take away the breath I was keeping for sun...  \n",
      "1     Black night is not right,  \\r\\nI don't feel so...  \n",
      "3     I'm going to rent myself a house  \\r\\nIn the s...  \n",
      "6     Beauty queen of only eighteen she  \\r\\nHad som...  \n",
      "8     Coming out of my cage  \\r\\nAnd I've been doing...  \n",
      "...                                                 ...  \n",
      "1988  Summertime....and the livin' is easy  \\r\\nFish...  \n",
      "1989  Now, since my baby left me  \\r\\nI've found a n...  \n",
      "1990  You ain't nothing but a hound dog, crying all ...  \n",
      "1991  Way down in Louisiana close to New Orleans  \\r...  \n",
      "1993  I found my thrill on Blueberry Hill  \\r\\nOn Bl...  \n",
      "\n",
      "[895 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#remove songs which were not in both datsets\n",
    "songs_to_remove = []\n",
    "for x, lyrics in enumerate(df['lyrics']):\n",
    "    if lyrics == '':\n",
    "        songs_to_remove.append(x)\n",
    "df.drop(songs_to_remove, axis = 0, inplace = True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e76e7",
   "metadata": {},
   "source": [
    "Further preprocessing of the lyrics itself. We remove the stopwords and punctuations with regex and stopwords from ntlk form df and df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db225637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alfrodo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Title          Top Genre  \\\n",
      "0               Sunrise    adult standards   \n",
      "1           Black Night         album rock   \n",
      "3         The Pretender  alternative metal   \n",
      "6     She Will Be Loved                pop   \n",
      "8        Mr. Brightside        modern rock   \n",
      "...                 ...                ...   \n",
      "1988         Summertime    adult standards   \n",
      "1989   Heartbreak Hotel    adult standards   \n",
      "1990          Hound Dog    adult standards   \n",
      "1991    Johnny B. Goode         blues rock   \n",
      "1993     Blueberry Hill    adult standards   \n",
      "\n",
      "                                                 lyrics  \n",
      "0     take away breath keep sunris appear morn look ...  \n",
      "1     black night right dont feel bright dont care s...  \n",
      "3     im go rent hous shade freeway gonna pack lunch...  \n",
      "6     beauti queen eighteen troubl alway help alway ...  \n",
      "8     come cage ive fine gotta gotta want start kiss...  \n",
      "...                                                 ...  \n",
      "1988  summertimeand livin easi fish jumpinand cotton...  \n",
      "1989  sinc babi left ive found new place dwell end l...  \n",
      "1990  aint noth hound dog cri time aint noth hound d...  \n",
      "1991  way louisiana close new orlean way back wood a...  \n",
      "1993  found thrill blueberri hill blueberri hill fou...  \n",
      "\n",
      "[895 rows x 3 columns]\n",
      "finished preprocessing of df\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "sw = stopwords.words('english')\n",
    "punc_regex = r'[^\\s\\w]' #searches for everything thats not a word or space\n",
    "stopword_regex = r'\\b{0}\\b'\n",
    "space_regex = r'\\s\\s+'\n",
    "newl_regex = r'\\n|\\r'\n",
    "\n",
    "#print(df3.iloc[1])\n",
    "\n",
    "#preprocessing of df\n",
    "for x, lyrics in enumerate(df['lyrics']):\n",
    "    txt = lyrics\n",
    "    txt = re.sub(punc_regex, '', txt) #remove punuctuations\n",
    "    for sword in sw:\n",
    "        txt = re.sub(stopword_regex.format(sword), '', txt, flags=re.IGNORECASE) #remove every stopword\n",
    "    txt = re.sub(newl_regex, '', txt)\n",
    "    stemmed_txt = ''\n",
    "    for word in txt.split():\n",
    "        stemmed_txt += str(stemmer.stem(word.lower())) + \" \"\n",
    "    \n",
    "    df['lyrics'].iloc[x] = stemmed_txt\n",
    "\n",
    "print(df)\n",
    "print('finished preprocessing of df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b5a71f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29764    turn tide calm angri sea alon decid write symp...\n",
      "34650    born nixon rais hell welfar child teamster dwe...\n",
      "38399    girl sometim seem dont say thing act like im g...\n",
      "4860     come your understand tell lie pretend believ s...\n",
      "30581    round drive crazi watch run make lazi your tri...\n",
      "                               ...                        \n",
      "56020    im human sometim make mistak forgiv im gonna t...\n",
      "56557    climb aboard butterfli take breez let worri fl...\n",
      "56185    woman enough take man like babi know got make ...\n",
      "7408     everybodi say let asham take round make differ...\n",
      "44614    learn love assembl way today tomorrow alway we...\n",
      "Name: text, Length: 10000, dtype: object\n",
      "finished preprocessing of df3\n"
     ]
    }
   ],
   "source": [
    "#preprocessing of df3\n",
    "for x, lyrics in enumerate(df3):\n",
    "    txt = lyrics\n",
    "    txt = re.sub(punc_regex, '', txt) #remove punuctuations\n",
    "    for sword in sw:\n",
    "        txt = re.sub(stopword_regex.format(sword), '', txt, flags=re.IGNORECASE) #remove every stopword\n",
    "    txt = re.sub(newl_regex, '', txt)\n",
    "    stemmed_txt = ''\n",
    "    for word in txt.split():\n",
    "        stemmed_txt += str(stemmer.stem(word.lower())) + \" \"\n",
    "        \n",
    "    df3.iloc[x] = stemmed_txt\n",
    "    \n",
    "print(df3)\n",
    "print('finished preprocessing of df3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "135a0530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          track_name    genre  \\\n",
      "0               mohabbat bhi jhoothi      pop   \n",
      "1                          i believe      pop   \n",
      "2                                cry      pop   \n",
      "3                           patricia      pop   \n",
      "4                 apopse eida oneiro      pop   \n",
      "...                              ...      ...   \n",
      "28367                10 million ways  hip hop   \n",
      "28368  ante up (robbin hoodz theory)  hip hop   \n",
      "28369                  whutcha want?  hip hop   \n",
      "28370                         switch  hip hop   \n",
      "28371                         r.i.p.  hip hop   \n",
      "\n",
      "                                                  lyrics  \n",
      "0      hold time feel break feel untru convinc speak ...  \n",
      "1      believ drop rain fall grow believ darkest nigh...  \n",
      "2      sweetheart send letter goodby secret feel bett...  \n",
      "3      kiss lip want stroll charm mambo chacha mering...  \n",
      "4      till darl till matter know till dream live apa...  \n",
      "...                                                  ...  \n",
      "28367  caus fuck leav scar tick tock clock come knock...  \n",
      "28368  mink thing chain ring braclet yap fame come fo...  \n",
      "28369  get ban get ban stick crack relax plan attack ...  \n",
      "28370  check check yeah yeah hear thing call switch g...  \n",
      "28371  remix killer aliv remix thriller trap bitch sp...  \n",
      "\n",
      "[28372 rows x 3 columns]\n",
      "finished preprocessing of df4\n"
     ]
    }
   ],
   "source": [
    "#preprocessing of df4\n",
    "for x, lyrics in enumerate(df4['lyrics']):\n",
    "    txt = lyrics\n",
    "    txt = re.sub(punc_regex, '', txt) #remove punuctuations\n",
    "    for sword in sw:\n",
    "        txt = re.sub(stopword_regex.format(sword), '', txt, flags=re.IGNORECASE) #remove every stopword\n",
    "    \n",
    "    txt = re.sub(newl_regex, '', txt)\n",
    "    stemmed_txt = ''\n",
    "    for word in txt.split():\n",
    "        stemmed_txt += str(stemmer.stem(word.lower())) + \" \"\n",
    "    df4['lyrics'].iloc[x] = stemmed_txt\n",
    "\n",
    "print(df4)\n",
    "print('finished preprocessing of df4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f43056ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████| 612/612 [00:00<00:00, 157kB/s]\n",
      "Downloading: 100%|█████████████████████████████| 116/116 [00:00<00:00, 34.9kB/s]\n",
      "Downloading: 100%|██████████████████████████| 39.3k/39.3k [00:00<00:00, 136kB/s]\n",
      "Downloading: 100%|██████████████████████████| 90.9M/90.9M [02:07<00:00, 712kB/s]\n",
      "Downloading: 100%|███████████████████████████| 53.0/53.0 [00:00<00:00, 14.9kB/s]\n",
      "Downloading: 100%|█████████████████████████████| 112/112 [00:00<00:00, 42.0kB/s]\n",
      "Downloading: 100%|████████████████████████████| 466k/466k [00:00<00:00, 688kB/s]\n",
      "Downloading: 100%|██████████████████████████████| 350/350 [00:00<00:00, 142kB/s]\n",
      "Downloading: 100%|██████████████████████████| 13.2k/13.2k [00:00<00:00, 109kB/s]\n",
      "Downloading: 100%|████████████████████████████| 232k/232k [00:00<00:00, 428kB/s]\n",
      "Downloading: 100%|██████████████████████████████| 349/349 [00:00<00:00, 122kB/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "103",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 103",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#display(df)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m topic_model \u001b[38;5;241m=\u001b[39m bertopic\u001b[38;5;241m.\u001b[39mBERTopic(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, embedding_model\u001b[38;5;241m=\u001b[39msentence_model,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m topics, probs \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlyrics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(topics)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bertopic/_bertopic.py:339\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[0;34m(self, documents, embeddings, y)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;241m=\u001b[39m select_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model,\n\u001b[1;32m    338\u001b[0m                                           language\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage)\n\u001b[0;32m--> 339\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDocument\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformed documents to Embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bertopic/_bertopic.py:2787\u001b[0m, in \u001b[0;36mBERTopic._extract_embeddings\u001b[0;34m(self, documents, method, verbose)\u001b[0m\n\u001b[1;32m   2785\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model\u001b[38;5;241m.\u001b[39membed_words(documents, verbose)\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2787\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2788\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2789\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong method for extracting document/word embeddings. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2790\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither choose \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as the method. \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bertopic/backend/_base.py:69\u001b[0m, in \u001b[0;36mBaseEmbedder.embed_documents\u001b[0;34m(self, document, verbose)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     56\u001b[0m                     document: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m     57\u001b[0m                     verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124;03m\"\"\" Embed a list of n words into an n-dimensional\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    matrix of embeddings\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bertopic/backend/_sentencetransformers.py:63\u001b[0m, in \u001b[0;36mSentenceTransformerBackend.embed\u001b[0;34m(self, documents, verbose)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     50\u001b[0m           documents: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m     51\u001b[0m           verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124;03m\"\"\" Embed a list of n documents/words into an n-dimensional\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    matrix of embeddings\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:157\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    155\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m length_sorted_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort([\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_length(sen) \u001b[38;5;28;01mfor\u001b[39;00m sen \u001b[38;5;129;01min\u001b[39;00m sentences])\n\u001b[0;32m--> 157\u001b[0m sentences_sorted \u001b[38;5;241m=\u001b[39m [sentences[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m length_sorted_idx]\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m    160\u001b[0m     sentences_batch \u001b[38;5;241m=\u001b[39m sentences_sorted[start_index:start_index\u001b[38;5;241m+\u001b[39mbatch_size]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:157\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    155\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m length_sorted_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort([\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_length(sen) \u001b[38;5;28;01mfor\u001b[39;00m sen \u001b[38;5;129;01min\u001b[39;00m sentences])\n\u001b[0;32m--> 157\u001b[0m sentences_sorted \u001b[38;5;241m=\u001b[39m [\u001b[43msentences\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m length_sorted_idx]\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m    160\u001b[0m     sentences_batch \u001b[38;5;241m=\u001b[39m sentences_sorted[start_index:start_index\u001b[38;5;241m+\u001b[39mbatch_size]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 103"
     ]
    }
   ],
   "source": [
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "#display(df)\n",
    "\n",
    "topic_model = bertopic.BERTopic(language='english', embedding_model=sentence_model,verbose=True)\n",
    "\n",
    "topics, probs = topic_model.fit_transform(df[\"lyrics\"])\n",
    "\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad17c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_model.get_topic_info())\n",
    "\n",
    "print(topic_model.get_topics()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded6eae",
   "metadata": {},
   "source": [
    "Tokenize the lyrics and create tagged Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39f23826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alfrodo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28372\n"
     ]
    }
   ],
   "source": [
    "tagged_data = []\n",
    "nltk.download('punkt')\n",
    "\n",
    "for i,d in enumerate(df4['lyrics']):\n",
    "    tokenized_words = nltk.tokenize.word_tokenize(d)\n",
    "    tagged_data.append(TaggedDocument(words=tokenized_words, tags=str(i)))\n",
    "\n",
    "print(len(tagged_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982a54b",
   "metadata": {},
   "source": [
    "Setting up the Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52e8276d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec model finished training\n"
     ]
    }
   ],
   "source": [
    "doc2vec_model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=1, epochs=30)\n",
    "doc2vec_model.build_vocab(tagged_data)\n",
    "doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n",
    "print(\"Doc2Vec model finished training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0446769",
   "metadata": {},
   "source": [
    "Building the DataLoader for the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5778176",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Token'] + ['Target']\n",
    "df_for_dataloader = pd.DataFrame(columns = columns)\n",
    "df_for_dataloader.set_index(columns)\n",
    "\n",
    "lookup_dict = {}\n",
    "token_list = []\n",
    "target_list = []\n",
    "i = 0\n",
    "for d in df4[\"genre\"]:\n",
    "    if d not in lookup_dict:\n",
    "        lookup_dict[d] = i\n",
    "        i+=1\n",
    "\n",
    "for index, df_row in df4.iterrows():\n",
    "    lyrics_tokenized = nltk.tokenize.word_tokenize(df_row[\"lyrics\"])\n",
    "    token = [doc2vec_model.infer_vector(lyrics_tokenized)]\n",
    "    #print(token)\n",
    "    one_hot_encoded_vector = []\n",
    "    for x in lookup_dict.keys():\n",
    "        if df_row[\"genre\"] == x:\n",
    "            one_hot_encoded_vector.append(1)\n",
    "        else:\n",
    "            one_hot_encoded_vector.append(0)\n",
    "    target = [np.array(one_hot_encoded_vector)] # should be genre\n",
    "    #row = pd.DataFrame([token + target], columns=['Token', 'Target'])\n",
    "    #df_for_dataloader = pd.concat([df_for_dataloader, row])\n",
    "    token_list.append(token)\n",
    "    target_list.append(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8b7b4",
   "metadata": {},
   "source": [
    "Splitting the dataset into train and test (80,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8197733e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22698\n",
      "5674\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = torch.utils.data.random_split(list(zip(token_list, target_list)), [0.8, 0.2])\n",
    "print(len(data_train))\n",
    "print(len(data_test))\n",
    "    \n",
    "dataloader_train = torch.utils.data.DataLoader(data_train, batch_size=32)\n",
    "dataloader_test = torch.utils.data.DataLoader(data_test, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b0da18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 968\n",
      "1 1090\n",
      "5 811\n",
      "0 1352\n",
      "6 184\n",
      "3 766\n",
      "4 503\n"
     ]
    }
   ],
   "source": [
    "genre_test_count_dict = {}\n",
    "for i in data_test:\n",
    "  if np.argmax(i[1]) in genre_test_count_dict:\n",
    "    genre_test_count_dict[np.argmax(i[1])] += 1\n",
    "  else:\n",
    "    genre_test_count_dict[np.argmax(i[1])] = 1\n",
    "\n",
    "for i in genre_test_count_dict:\n",
    "  print(i, genre_test_count_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b771ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu') #torch.device('cuda' if torch.cuda.is_available() else 'cpu') For faster training in the end\n",
    "\n",
    "class SongModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SongModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layer_1 =   nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.layer_2 =   nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.layer_out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax =   nn.Softmax()\n",
    "        self.sigmoid =   nn.Sigmoid()\n",
    "        self.tanh =      nn.Tanh()\n",
    "\n",
    "        nn.init.xavier_uniform_(self.layer_1.weight)\n",
    "        nn.init.zeros_(self.layer_1.bias)\n",
    "        nn.init.xavier_uniform_(self.layer_2.weight)\n",
    "        nn.init.zeros_(self.layer_2.bias)\n",
    "        nn.init.xavier_uniform_(self.layer_out.weight)\n",
    "        nn.init.zeros_(self.layer_out.bias)\n",
    "\n",
    "\n",
    "    def forward(self, d):\n",
    "        x = self.sigmoid(self.layer_1(d))\n",
    "        # x = self.tanh(self.layer_2(x))\n",
    "        x = self.softmax(self.layer_out(x))\n",
    "        return x\n",
    "\n",
    "model = SongModel(dataloader_train.dataset[0][0][0].size, dataloader_train.dataset[0][1][0].size).to(device)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10d742ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    iterations = 0\n",
    "    acc = 0\n",
    "    count = 0\n",
    "    for sample in dataloader:\n",
    "        model_input = sample[0][0]\n",
    "        should = sample[1][0]\n",
    "        predict = model(model_input)\n",
    "        for sample_index in range(len(sample)):\n",
    "            count += 1\n",
    "            predict_e = predict[sample_index].data.numpy()\n",
    "            should_e = should[sample_index].numpy()\n",
    "            if np.where(predict_e == np.amax(predict_e))[0][0] == np.where(should_e == np.amax(should_e))[0][0]:\n",
    "                acc += 1\n",
    "        loss = loss_fn(predict, should.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        iterations += 1\n",
    "        # if t % 10 == 0:\n",
    "    print(f\"\\tTrain - \\tLoss: {total_loss / len(dataloader):3.10}, Acc: {acc *100 / count:3.5}%\", end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d220ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    acc = 0\n",
    "    count = 0\n",
    "    genre_accuracy = {}\n",
    "    for x, key in enumerate(lookup_dict):\n",
    "        genre_accuracy[x] = 0\n",
    "    \n",
    "    for sample in dataloader:\n",
    "        model_input = sample[0][0]\n",
    "        should = sample[1][0]\n",
    "        predict = model(model_input)\n",
    "        for sample_index in range(len(sample)):\n",
    "            #print(len(sample))\n",
    "            count += 1\n",
    "            predict_e = predict[sample_index].data.numpy()\n",
    "            should_e = should[sample_index].numpy()\n",
    "            if np.where(predict_e == np.amax(predict_e))[0][0] == np.where(should_e == np.amax(should_e))[0][0]:\n",
    "                acc += 1\n",
    "                genre_accuracy[np.where(should_e == np.amax(should_e))[0][0]] += 1\n",
    "                \n",
    "        loss = loss_fn(predict, should.float())\n",
    "        total_loss += loss\n",
    "        # if t % 10 == 0:\n",
    "    #print(acc, count)\n",
    "    print(f\"\\tEvaluation - \\tLoss: {total_loss / len(dataloader):3.10}, Acc: {acc *100 / count:3.5}%\", end=\"\\n\")\n",
    "    #print(genre_accuracy)\n",
    "    #for genre in lookup_dict:\n",
    "        #print(f\"\\t {genre}:\\t\\t {round(genre_accuracy[lookup_dict[genre]] * 100 / genre_test_count_dict[np.where(should_e == np.amax(should_e))[0][0]],3)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ebde06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "\tTrain - \tLoss: 1.85376358, Acc: 31.901%\n",
      "\tEvaluation - \tLoss: 1.850006938, Acc: 25.562%\n",
      "Epoch 1:\n",
      "\tTrain - \tLoss: 1.833557963, Acc: 33.732%\n",
      "\tEvaluation - \tLoss: 1.839122415, Acc: 26.124%\n",
      "Epoch 2:\n",
      "\tTrain - \tLoss: 1.824562788, Acc: 35.07%\n",
      "\tEvaluation - \tLoss: 1.833694339, Acc: 27.809%\n",
      "Epoch 3:\n",
      "\tTrain - \tLoss: 1.819427013, Acc: 35.423%\n",
      "\tEvaluation - \tLoss: 1.830513239, Acc: 28.371%\n",
      "Epoch 4:\n",
      "\tTrain - \tLoss: 1.816101551, Acc: 35.352%\n",
      "\tEvaluation - \tLoss: 1.828421831, Acc: 28.652%\n",
      "Epoch 5:\n",
      "\tTrain - \tLoss: 1.813737512, Acc: 35.845%\n",
      "\tEvaluation - \tLoss: 1.826940417, Acc: 29.213%\n",
      "Epoch 6:\n",
      "\tTrain - \tLoss: 1.811952233, Acc: 35.986%\n",
      "\tEvaluation - \tLoss: 1.825835466, Acc: 29.213%\n",
      "Epoch 7:\n",
      "\tTrain - \tLoss: 1.81054616, Acc: 35.704%\n",
      "\tEvaluation - \tLoss: 1.824979663, Acc: 29.213%\n",
      "Epoch 8:\n",
      "\tTrain - \tLoss: 1.809398055, Acc: 35.563%\n",
      "\tEvaluation - \tLoss: 1.824297309, Acc: 29.494%\n",
      "Epoch 9:\n",
      "\tTrain - \tLoss: 1.808443666, Acc: 35.493%\n",
      "\tEvaluation - \tLoss: 1.823740125, Acc: 29.494%\n",
      "Epoch 10:\n",
      "\tTrain - \tLoss: 1.807630301, Acc: 35.423%\n",
      "\tEvaluation - \tLoss: 1.823278904, Acc: 29.775%\n",
      "Epoch 11:\n",
      "\tTrain - \tLoss: 1.806929588, Acc: 35.352%\n",
      "\tEvaluation - \tLoss: 1.822890043, Acc: 30.056%\n",
      "Epoch 12:\n",
      "\tTrain - \tLoss: 1.80631721, Acc: 35.563%\n",
      "\tEvaluation - \tLoss: 1.822557211, Acc: 30.056%\n",
      "Epoch 13:\n",
      "\tTrain - \tLoss: 1.80577302, Acc: 35.493%\n",
      "\tEvaluation - \tLoss: 1.822270274, Acc: 30.056%\n",
      "Epoch 14:\n",
      "\tTrain - \tLoss: 1.805291295, Acc: 35.352%\n",
      "\tEvaluation - \tLoss: 1.822019577, Acc: 30.056%\n",
      "Epoch 15:\n",
      "\tTrain - \tLoss: 1.804855943, Acc: 35.282%\n",
      "\tEvaluation - \tLoss: 1.821800113, Acc: 30.056%\n",
      "Epoch 16:\n",
      "\tTrain - \tLoss: 1.804461122, Acc: 35.282%\n",
      "\tEvaluation - \tLoss: 1.821606278, Acc: 30.056%\n",
      "Epoch 17:\n",
      "\tTrain - \tLoss: 1.804101229, Acc: 35.352%\n",
      "\tEvaluation - \tLoss: 1.82143271, Acc: 30.337%\n",
      "Epoch 18:\n",
      "\tTrain - \tLoss: 1.803774238, Acc: 35.493%\n",
      "\tEvaluation - \tLoss: 1.821276665, Acc: 30.337%\n",
      "Epoch 19:\n",
      "\tTrain - \tLoss: 1.803470373, Acc: 35.634%\n",
      "\tEvaluation - \tLoss: 1.82113719, Acc: 30.337%\n",
      "Epoch 20:\n",
      "\tTrain - \tLoss: 1.803189993, Acc: 35.634%\n",
      "\tEvaluation - \tLoss: 1.82101059, Acc: 30.618%\n",
      "Epoch 21:\n",
      "\tTrain - \tLoss: 1.802929521, Acc: 35.634%\n",
      "\tEvaluation - \tLoss: 1.820895076, Acc: 30.618%\n",
      "Epoch 22:\n",
      "\tTrain - \tLoss: 1.802687407, Acc: 35.634%\n",
      "\tEvaluation - \tLoss: 1.820789456, Acc: 30.618%\n",
      "Epoch 23:\n",
      "\tTrain - \tLoss: 1.802461147, Acc: 35.563%\n",
      "\tEvaluation - \tLoss: 1.820692539, Acc: 30.618%\n",
      "Epoch 24:\n",
      "\tTrain - \tLoss: 1.802249551, Acc: 35.704%\n",
      "\tEvaluation - \tLoss: 1.820602894, Acc: 30.618%\n",
      "Epoch 25:\n",
      "\tTrain - \tLoss: 1.802048206, Acc: 35.634%\n",
      "\tEvaluation - \tLoss: 1.820520163, Acc: 30.618%\n",
      "Epoch 26:\n",
      "\tTrain - \tLoss: 1.80185926, Acc: 35.493%\n",
      "\tEvaluation - \tLoss: 1.820443869, Acc: 30.618%\n",
      "Epoch 27:\n",
      "\tTrain - \tLoss: 1.801679969, Acc: 35.493%\n",
      "\tEvaluation - \tLoss: 1.820372343, Acc: 30.618%\n",
      "Epoch 28:\n",
      "\tTrain - \tLoss: 1.801511407, Acc: 35.493%\n",
      "\tEvaluation - \tLoss: 1.820305109, Acc: 30.618%\n",
      "Epoch 29:\n",
      "\tTrain - \tLoss: 1.801347733, Acc: 35.493%\n",
      "\tEvaluation - \tLoss: 1.820240378, Acc: 30.618%\n",
      "Epoch 30:\n",
      "\tTrain - \tLoss: 1.801194072, Acc: 35.352%\n",
      "\tEvaluation - \tLoss: 1.820177436, Acc: 30.618%\n",
      "Epoch 31:\n",
      "\tTrain - \tLoss: 1.801043749, Acc: 35.211%\n",
      "\tEvaluation - \tLoss: 1.820113301, Acc: 30.618%\n",
      "Epoch 32:\n",
      "\tTrain - \tLoss: 1.800894737, Acc: 35.211%\n",
      "\tEvaluation - \tLoss: 1.820040703, Acc: 30.618%\n",
      "Epoch 33:\n",
      "\tTrain - \tLoss: 1.800741196, Acc: 35.211%\n",
      "\tEvaluation - \tLoss: 1.819929957, Acc: 30.337%\n",
      "Epoch 34:\n",
      "\tTrain - \tLoss: 1.80054009, Acc: 35.141%\n",
      "\tEvaluation - \tLoss: 1.819634557, Acc: 30.337%\n",
      "Epoch 35:\n",
      "\tTrain - \tLoss: 1.800165534, Acc: 35.141%\n",
      "\tEvaluation - \tLoss: 1.818974853, Acc: 30.337%\n",
      "Epoch 36:\n",
      "\tTrain - \tLoss: 1.799651384, Acc: 35.211%\n",
      "\tEvaluation - \tLoss: 1.818371058, Acc: 30.337%\n",
      "Epoch 37:\n",
      "\tTrain - \tLoss: 1.799107671, Acc: 35.211%\n",
      "\tEvaluation - \tLoss: 1.817809343, Acc: 30.618%\n",
      "Epoch 38:\n",
      "\tTrain - \tLoss: 1.79852736, Acc: 35.211%\n",
      "\tEvaluation - \tLoss: 1.817239642, Acc: 30.618%\n",
      "Epoch 39:\n",
      "\tTrain - \tLoss: 1.797905326, Acc: 35.282%\n",
      "\tEvaluation - \tLoss: 1.816651821, Acc: 30.337%\n",
      "Epoch 40:\n",
      "\tTrain - \tLoss: 1.797248483, Acc: 35.352%\n",
      "\tEvaluation - \tLoss: 1.816043615, Acc: 30.618%\n",
      "Epoch 41:\n",
      "\tTrain - \tLoss: 1.796556115, Acc: 35.704%\n",
      "\tEvaluation - \tLoss: 1.815417171, Acc: 30.618%\n",
      "Epoch 42:\n",
      "\tTrain - \tLoss: 1.795840621, Acc: 35.634%\n",
      "\tEvaluation - \tLoss: 1.814775944, Acc: 30.618%\n",
      "Epoch 43:\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "for t in range(epochs):\n",
    "    print(\"Epoch \" + str(t) + \":\")\n",
    "    train(model, dataloader_train, optimizer, loss_fn)\n",
    "    test(model, dataloader_test, loss_fn)\n",
    "    torch.save(model.state_dict(), \"model_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097806e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_xxp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
